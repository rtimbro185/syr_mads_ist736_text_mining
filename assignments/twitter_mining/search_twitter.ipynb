{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TITLE\n",
    "\n",
    "\n",
    "Ryan Timbrook (RTIMBROO)  \n",
    "DATE:\n",
    "Topic:  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Objective\n",
    "_____________________________________________________________________________________________\n",
    "place object and info here  \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Findings / Recommendations\n",
    "place findings and recommendations here  \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___________________________________________________________________________________________\n",
    "## FOR RUNNING IN GOOGLE COLAB ONLY ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# toggle for working with colab\n",
    "isColab = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#*ONLY RUN WHEN WORKING ON COLAB*\n",
    "#===================================================\n",
    "# mount google drive for working in colab\n",
    "\n",
    "#from google.colab import drive\n",
    "#drive.mount('/content/gdrive', force_remount=True)\n",
    "\n",
    "# working within colab, set base working directory\n",
    "#base_dir = \"./gdrive/My Drive/IST707_PRJ_Realestate/buy_rent_sell/\"\n",
    "\n",
    "# validate directory mapping\n",
    "#ls f'{base_dir}'\n",
    "\n",
    "# upload custome python files\n",
    "#from google.colab import files\n",
    "#uploaded_files = files.upload()\n",
    "\n",
    "# print files uploaded\n",
    "#for f in uploaded_files.keys():\n",
    "#  print(f'file name: {f}')\n",
    "\n",
    "#isColab = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______________________________________________________________________________________________\n",
    "### Coding Environment Setup\n",
    "Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages for analysis and modeling\n",
    "import pandas as pd #data frame operations\n",
    "import numpy as np #arrays and math functions\n",
    "from scipy.stats import uniform #for training and test splits\n",
    "from scipy.stats import gaussian_kde as kde # for resampling dataset\n",
    "from scipy import stats #\n",
    "import statsmodels.formula.api as smf #R-like model specification\n",
    "import matplotlib.pyplot as plt #2D plotting\n",
    "%matplotlib inline\n",
    "import seaborn as sns #\n",
    "import requests\n",
    "import os\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# packages for twitter\n",
    "import tweepy as tw\n",
    "from tweepy import OAuthHandler\n",
    "import json\n",
    "from tweepy import Stream\n",
    "from tweepy.streaming import StreamListener\n",
    "\n",
    "# Twython packages for twitter\n",
    "from twython import Twython\n",
    "\n",
    "# packages for NLTK\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import re\n",
    "\n",
    "import sys\n",
    "from os import path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custome python packages\n",
    "import rtimbroo_utils as br             # custome python helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set global properties\n",
    "notebook_file_name = 'hw1_twitter_sentiment'\n",
    "report_file_name = 'HW_1_Timbrook_Ryan'\n",
    "app_name = 'AI_Public_Sentiment'\n",
    "log_level = 10 # 10-DEBUG, 20-INFO, 30-WARNING, 40-ERROR, 50-CRITICAL\n",
    "\n",
    "# setup working directory structure\n",
    "# set global properties\n",
    "if not isColab:\n",
    "    dataDir = './data'\n",
    "    outputDir = './output'\n",
    "    configDir = './config'\n",
    "    logOutDir = './logs'\n",
    "    imageDir = './images'\n",
    "    modelDir = './models'\n",
    "    corpusDir = './corpus'\n",
    "else:\n",
    "    # working within colab\n",
    "    dataDir = f'{base_dir}data'\n",
    "    outputDir = f'{base_dir}output'\n",
    "    configDir = f'{base_dir}config'\n",
    "    logOutDir = f'{base_dir}logs'\n",
    "    imageDir = f'{base_dir}images'\n",
    "    modelDir = f'{base_dir}models'\n",
    "    corpusDir = f'{base_dir}corpus'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create base output directories if they don't exist\n",
    "if not os.path.exists(outputDir): os.mkdir(outputDir)\n",
    "if not os.path.exists(logOutDir): os.mkdir(logOutDir)\n",
    "if not os.path.exists(imageDir): os.mkdir(imageDir)\n",
    "if not os.path.exists(modelDir): os.mkdir(modelDir)\n",
    "if not os.path.exists(dataDir): os.mkdir(dataDir)\n",
    "if not os.path.exists(configDir): os.mkdir(configDir)\n",
    "if not os.path.exists(corpusDir): os.mkdir(corpusDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a logger for troubleshooting / data exploration\n",
    "logger = br.getFileLogger(logOutDir+'/',app_name,level=log_level)\n",
    "np.random.seed(42) # NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# twitter API secret/key\n",
    "#import json\n",
    "\n",
    "# Enter your keys/secrets as strings in the following fields\n",
    "#credentials = {}\n",
    "#credentials['CONSUMER_KEY'] = ''\n",
    "#credentials['CONSUMER_SECRET'] = ''\n",
    "#credentials['ACCESS_TOKEN'] = '-'\n",
    "#credentials['ACCESS_SECRET'] = ''\n",
    "\n",
    "# Save the credentials object to file\n",
    "#with open(f'{configDir}/twitter_credentials.json', \"w+\") as file:\n",
    "    #json.dump(credentials, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. OBTAIN the data   \n",
    "________________________________________________________________________________________________\n",
    "Import external datasets for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load twitter credentials\n",
    "with open(f'{configDir}/twitter_credentials.json', 'r') as f:\n",
    "    tw_cred = json.load(f)\n",
    "\n",
    "# instantiate object\n",
    "py_tweets = Twython(tw_cred['CONSUMER_KEY'],tw_cred['CONSUMER_SECRET'],tw_cred['ACCESS_TOKEN'],tw_cred['ACCESS_SECRET'])\n",
    "\n",
    "# setup base twitter search query\n",
    "search_terms = 'Artificial+Intelligence OR machine+learning'\n",
    "search_start_date = '2019-10-9'\n",
    "# add filters to search criteria\n",
    "filtered_search_terms = search_terms + \" -filter:retweets\"\n",
    "# number of tweets to return\n",
    "num_tweets = 10000\n",
    "\n",
    "# query\n",
    "base_tw_query = {\n",
    "    'q':filtered_search_terms,\n",
    "    'since':search_start_date,\n",
    "    'count':num_tweets,\n",
    "    'lang':'en',\n",
    "}\n",
    "\n",
    "# output file names\n",
    "tweet_filename=f'{dataDir}/{search_start_date}_ai_tweet_text.txt'\n",
    "raw_filename=f'{dataDir}/{search_start_date}_ai_tweet_raw.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tw_cred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "# search tweets\n",
    "tweets_dict = {'user':[],'date':[],'text':[],'favorite_count':[]}\n",
    "\n",
    "tweet_iterations=10\n",
    "with open(raw_filename,'w+') as f:\n",
    "    for i in range(1,tweet_iterations):\n",
    "        try:\n",
    "            for status in py_tweets.search(**base_tw_query)['statuses']:\n",
    "                tweets_dict['user'].append(status['user']['screen_name'])\n",
    "                tweets_dict['date'].append(status['created_at'])\n",
    "                tweets_dict['text'].append(status['text'])\n",
    "                tweets_dict['favorite_count'].append(status['favorite_count'])\n",
    "            time.sleep(10)\n",
    "        except BaseException as be:\n",
    "            print(f'Caught Exception:\\n {be}')\n",
    "            \n",
    "    tweets_df = pd.DataFrame(tweets_dict)\n",
    "    tweets_df.sort_values(by='date', inplace=True, ascending=False)\n",
    "    #tweets_j = json.load(tweets_dict)\n",
    "    json.dump(tweets_dict,f)\n",
    "    print(tweets_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load twitter raw \n",
    "with open(f'{dataDir}/{search_start_date}_ai_tweet_raw.txt', 'r') as f:\n",
    "    j_tweets = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_j_df = pd.DataFrame(j_tweets)\n",
    "tweets_j_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = tweets_j_df.sort_values(by='date', inplace=True, ascending=False)\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tweets_j_df.text.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev save_to_file(self,tweet):\n",
    "    with open(f'{dataDir}/saved_tweets.csv','a') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(list(tweet.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save tweets off to a file\n",
    "tweet_file_name = 'ai_tweets.txt'\n",
    "raw_tweet_file_name = 'raw_ai_tweets.txt'\n",
    "failed_tweets_text = []\n",
    "failed_count = 0\n",
    "\n",
    "try:\n",
    "    with open(dataDir+'/'+tweet_file_name, 'w+') as f:\n",
    "        for t in tweets:\n",
    "            try:\n",
    "                f.write(t.text+'\\n')\n",
    "            except:\n",
    "                failed_count = failed_count + 1\n",
    "                failed_tweets_text.append(t.text)\n",
    "                f.write('\\n')\n",
    "                print(f'Failed to write tweet:\\n{t.text}')\n",
    "            \n",
    "except BaseException as be:\n",
    "    print(f'Caught BaseException:\\n {be}')\n",
    "    pass\n",
    "\n",
    "print(f'Failed to write {failed_count}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# read tweets text file\n",
    "line_count=0\n",
    "hash_count=0\n",
    "word_count=0\n",
    "tweet_index=0\n",
    "bag_of_words=[]\n",
    "bag_of_hashes=[]\n",
    "bag_of_links=[]\n",
    "tweets_cleaned={}\n",
    "tweet_hashs={}\n",
    "tweet_links={}\n",
    "\n",
    "# setup some regular expressions\n",
    "regex1=re.compile('^#.+')\n",
    "regex2=re.compile('[^\\W\\d]') #no numbers\n",
    "regex3=re.compile('^http*')\n",
    "regex4=re.compile('.+\\..+')\n",
    "\n",
    "with open(f'{dataDir}/{tweet_file_name}') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if line == '\\n': \n",
    "            tweet_index+=1\n",
    "            \n",
    "        #print(line)\n",
    "        #line = line.replace('\\n','')\n",
    "        line = line.strip()\n",
    "        line_count+=1\n",
    "        \n",
    "        tweetSplitter = TweetTokenizer(strip_handles=True, reduce_len=True)\n",
    "        tweetTokens = tweetSplitter.tokenize(line)\n",
    "        \n",
    "        for token in tweetTokens:\n",
    "            if(len(token)>2):\n",
    "                logger.debug(f'Token: {token}')\n",
    "                \n",
    "                if((re.match(regex1,token))):\n",
    "                    logger.debug(f'Token: {token} | matched regex1 {regex1} - will be added to bag of hashes list')\n",
    "                    n_token=token[1:]\n",
    "                    bag_of_hashes.append(n_token) # capture all hashtags\n",
    "                    hash_count+=1\n",
    "                elif(re.match(regex2,token)):\n",
    "                    logger.debug(f'Token: {token} | matched regex2 {regex2}')\n",
    "                    if(re.match(regex3,token) or re.match(regex4,token)):\n",
    "                        logger.debug(f'Token: {token} | matched regex3 {regex3} or regex4 {regex4} - will be added to bag of links list')\n",
    "                        bag_of_links.append(token) # capture all url links\n",
    "                    else:\n",
    "                        logger.debug(f'Token: {token} | did not match any of the regex patterns - will be added to bag of words list')\n",
    "                        bag_of_words.append(token) # capture all words\n",
    "                        word_count+=1\n",
    "                else:\n",
    "                    logger.debug(f'Token: {token} | did not match regex2 {regex2} | contains numbers')\n",
    "                    pass\n",
    "            else:\n",
    "                logger.debug(f'Token: {token} | is less than 2')\n",
    "                pass\n",
    "        # capture tweet index\n",
    "        tweets_cleaned[tweet_index] = bag_of_words\n",
    "        tweet_hashs[tweet_index] = bag_of_hashes\n",
    "        tweet_links[tweet_index] = bag_of_links\n",
    "        \n",
    "        #--End For Loop over tokens\n",
    "    #--End For loop over lines\n",
    "\n",
    "\n",
    "logger.info(f'bag_of_words count: {len(bag_of_words)} | bag_of_hashes count: {len(bag_of_hashes)} | bag_of_links count: {len(bag_of_links)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_links.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(bag_of_links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 SCRUB / CLEAN\n",
    "Clean and perform initial transformations steps of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform scrubbing and cleaning techniques\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Initial EXPLORE\n",
    "Explore the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform exploratory data analysis techiques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. MODEL\n",
    "_________________________________________________________________________________________________\n",
    "Create models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Model Analysis\n",
    "perform model creation and validation techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Model Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Model Build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Model Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Validation\n",
    "Perform model validations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### iNterpret\n",
    "Interpret the model results, make knowledge based recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform interpretation steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report Generation\n",
    "Execute the below cell to create an HTML report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for report auto generation\n",
    "# testing markup report generation\n",
    "from nbconvert import HTMLExporter\n",
    "import codecs\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "stamp = datetime.date.today().strftime(\"%m-%d-%Y\")\n",
    "exporter = HTMLExporter(template_file='report.tpl')\n",
    "output, resources = exporter.from_filename(notebook_file_name)\n",
    "new_fnw = report_file_name\n",
    "codecs.open(new_fnw, 'w',encoding='utf-8').write(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
