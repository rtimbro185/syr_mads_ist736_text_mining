{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IST 736 Text Mining\n",
    "### Mine Twitter for Data\n",
    "\n",
    "\n",
    "Ryan Timbrook (RTIMBROO)  \n",
    "DATE:10/18/2019<br>\n",
    "Topic: <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Objective\n",
    "_____________________________________________________________________________________________\n",
    "In this assignment, you have the freedom to find an existing text corpus, or create a new text corpus of your interest. If you are creating a new corpus, make sure it is not too large and thus too time-consuming to create.  \n",
    " \n",
    "Then you will vectorize the text corpus using any tool that you are comfortable with: Weka, R, Python, etc. Explain the decisions you made during the vectorization process, e.g., did you merge lower- and uppercase? Then you will explore the text vectors and see if you can find anything interesting. The lectures showed some examples of comparative analysis and trend analysis. But you have the freedom to define what would be interesting patterns as long as you can explain it in a sensible way.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Findings / Recommendations\n",
    "place findings and recommendations here  \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# toggle for working with colab\n",
    "isColab = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#*ONLY RUN WHEN WORKING ON COLAB*\n",
    "#===================================================\n",
    "# mount google drive for working in colab\n",
    "\n",
    "#from google.colab import drive\n",
    "#drive.mount('/content/gdrive', force_remount=True)\n",
    "\n",
    "# working within colab, set base working directory\n",
    "#base_dir = \"./gdrive/My Drive/IST707_PRJ_Realestate/buy_rent_sell/\"\n",
    "\n",
    "# validate directory mapping\n",
    "#ls f'{base_dir}'\n",
    "\n",
    "# upload custome python files\n",
    "#from google.colab import files\n",
    "#uploaded_files = files.upload()\n",
    "\n",
    "# print files uploaded\n",
    "#for f in uploaded_files.keys():\n",
    "#  print(f'file name: {f}')\n",
    "\n",
    "#isColab = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______________________________________________________________________________________________\n",
    "### Coding Environment Setup\n",
    "Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages for analysis and modeling\n",
    "import pandas as pd #data frame operations\n",
    "import numpy as np #arrays and math functions\n",
    "import matplotlib.pyplot as plt #2D plotting\n",
    "%matplotlib inline\n",
    "import seaborn as sns #\n",
    "import requests\n",
    "import os\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# packages for twitter\n",
    "import tweepy as tw\n",
    "import codecs\n",
    "from tweepy import OAuthHandler\n",
    "import json\n",
    "from tweepy import Stream\n",
    "from tweepy.streaming import StreamListener\n",
    "\n",
    "# packages for NLTK\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "import re\n",
    "\n",
    "import sys\n",
    "from os import path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custome python packages\n",
    "import rtimbroo_utils as br             # custome python helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set global properties\n",
    "notebook_file_name = 'pull_twitter_data'\n",
    "report_file_name = 'Twitter_Data_Collection'\n",
    "app_name = 'Twitter_Data_Collection'\n",
    "log_level = 10 # 10-DEBUG, 20-INFO, 30-WARNING, 40-ERROR, 50-CRITICAL\n",
    "\n",
    "# setup working directory structure\n",
    "# set global properties\n",
    "if not isColab:\n",
    "    dataDir = './data'\n",
    "    outputDir = './output'\n",
    "    configDir = './config'\n",
    "    logOutDir = './logs'\n",
    "    imageDir = './images'\n",
    "    modelDir = './models'\n",
    "    corpusDir = './corpus'\n",
    "else:\n",
    "    # working within colab\n",
    "    dataDir = f'{base_dir}data'\n",
    "    outputDir = f'{base_dir}output'\n",
    "    configDir = f'{base_dir}config'\n",
    "    logOutDir = f'{base_dir}logs'\n",
    "    imageDir = f'{base_dir}images'\n",
    "    modelDir = f'{base_dir}models'\n",
    "    corpusDir = f'{base_dir}corpus'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create base output directories if they don't exist\n",
    "if not os.path.exists(outputDir): os.mkdir(outputDir)\n",
    "if not os.path.exists(logOutDir): os.mkdir(logOutDir)\n",
    "if not os.path.exists(imageDir): os.mkdir(imageDir)\n",
    "if not os.path.exists(modelDir): os.mkdir(modelDir)\n",
    "if not os.path.exists(dataDir): os.mkdir(dataDir)\n",
    "if not os.path.exists(configDir): os.mkdir(configDir)\n",
    "if not os.path.exists(corpusDir): os.mkdir(corpusDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a logger for troubleshooting / data exploration\n",
    "logger = br.getFileLogger(logOutDir+'/',app_name,level=log_level)\n",
    "np.random.seed(42) # NumPy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. OBTAIN the data   \n",
    "________________________________________________________________________________________________\n",
    "* Step 1: [Get Active NFL Players List](https://www.lineups.com/nfl/rosters)\n",
    "    * \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load twitter credentials\n",
    "with open(f'{configDir}/twitter_credentials.json', 'r') as f:\n",
    "    tw_cred = json.load(f)\n",
    "\n",
    "# instantiate tweepy object\n",
    "auth = OAuthHandler(tw_cred['CONSUMER_KEY'], tw_cred['CONSUMER_SECRET'])\n",
    "auth.set_access_token(tw_cred['ACCESS_TOKEN'], tw_cred['ACCESS_SECRET'])\n",
    "api = tw.API(auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup base twitter search query\n",
    "search_terms = 'Artificial+Intelligence OR machine+learning'\n",
    "search_start_date = '2019-10-11'\n",
    "# add filters to search criteria\n",
    "filtered_search_terms = search_terms + \" -filter:retweets\"\n",
    "# number of tweets to return\n",
    "num_tweets = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use .Cursor() to search twitter for tweets containing the search term\n",
    "tweets = tw.Cursor(api.search,q=filtered_search_terms,lang='en',since=search_start_date).items(num_tweets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[e for e in tweets_df.target.unique()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df_text = tweets_df.drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 SCRUB / CLEAN\n",
    "Perform vectorization tasks\n",
    "\n",
    "Goal: Identify public sentiment toward AI in social media<br>\n",
    "Each Tweet is considered an individual document<br>\n",
    "\n",
    "Determin **what to count** and **how to count it**<br>\n",
    "\n",
    "Basic text preparation pipeline:\n",
    "\n",
    "* Load the raw text.\n",
    "* Split into tokens.\n",
    "* Convert to lowercase. -> not for sentiment analysis\n",
    "* Remove punctuation from each token.\n",
    "* Filter out remaining tokens that are not alphabetic.\n",
    "* Filter out tokens that are stop words.\n",
    "* Perform stemming -> [nltk reference](https://pythonprogramming.net/stemming-nltk-tutorial/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global variables\n",
    "initial_words_count = 0\n",
    "cleaned_words_count = 0\n",
    "tweets_count = 0\n",
    "feature_thres = 2\n",
    "rare_thres = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordcloud_draw(data, color='black', width=1000, height=750, max_font_size=50, max_words=100):\n",
    "    words = ' '.join([word for word in data])\n",
    "    #cleaned_word = \" \".join([word for word in words])\n",
    "    wordcloud = WordCloud(stopwords=STOPWORDS,\n",
    "                    background_color=color,\n",
    "                    width=width,\n",
    "                    height=height,\n",
    "                    max_font_size=max_font_size,\n",
    "                    max_words=max_words,\n",
    "                     ).generate(words)\n",
    "    plt.figure(1,figsize=(10.5, 7))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw tweet text has newline characters. this process groups the tweet lines together and indexes the tweet in a dictionary object\n",
    "tweet_index=0\n",
    "temp_index=0\n",
    "tweets_dict = {0:[]}\n",
    "tweets_sents = []\n",
    "prior_line_len = 0\n",
    "current_line_len = 0\n",
    "next_line_len = 0\n",
    "max_line_len = len(ai_tweets)\n",
    "\n",
    "for i, line in enumerate(ai_tweets):\n",
    "  \n",
    "    #print(f'line: {i} | Current Tweet Text: {ai_tweets[i]} | Text length: {len(ai_tweets[i])}')\n",
    "    current_line_len = len(ai_tweets[i])\n",
    "    \n",
    "    if i > 0: \n",
    "        #print(f'line: {i} | Prior Tweet Text: {ai_tweets[i-1]} | Text length: {len(ai_tweets[i-1])}')\n",
    "        prior_line_len = len(ai_tweets[i-1])\n",
    "    \n",
    "    #print(f'prior_line_len = {prior_line_len} | current_line_len = {current_line_len}')\n",
    "    \n",
    "    # next line possible to be in same tweet text\n",
    "    if (prior_line_len > 1 and current_line_len > 1) or (current_line_len > 1):\n",
    "        #print(f'In condition 2: prior_line_len > 1 and current_line_len > 1')\n",
    "        tweets_sents.append(line)\n",
    "        pass\n",
    "    \n",
    "    # questionable if multiple newline spaces show up\n",
    "    if (current_line_len == 1 and prior_line_len > 1) or (current_line_len == 1 and prior_line_len == 1):\n",
    "        #print(f'In condition 3: current_line_len == 1 and prior_line_len > 1')\n",
    "\n",
    "        tweets_dict[tweet_index] = tweets_sents # \n",
    "        tweets_sents = []\n",
    "        # new tweet text\n",
    "        tweet_index+=1\n",
    "        pass\n",
    "    \n",
    "    #print(f'Tweet:{tweet_index} | text:{line}')\n",
    "    #if tweet_index > 10: break;\n",
    "        \n",
    "len(tweets_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each tweet, clean the multiple parts and create one senteces per tweet. this will be the document to tokenize\n",
    "#tweets_dict\n",
    "tweets_text_flat = {}\n",
    "\n",
    "for k,v in tweets_dict.items():\n",
    "    line = []\n",
    "    sent = ''\n",
    "    #print(k)\n",
    "    #print(v)\n",
    "    \n",
    "    for e in v:\n",
    "        e = e.replace('\\n','')\n",
    "        sent = sent+e\n",
    "    \n",
    "    if len(sent) > 1:\n",
    "        line.append(sent.strip())\n",
    "        tweets_text_flat[k] = line\n",
    "\n",
    "#line\n",
    "tweets_count = len(tweets_text_flat)\n",
    "logger.info(f'Total Tweets (Documents) Count: {tweets_count}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Split Tweet text into it's tokens(feature words) using [NLTK TweetTokenizer](https://www.nltk.org/api/nltk.tokenize.html)**<br>\n",
    "\n",
    "**Parameters**\n",
    "* strip_handles=True -> Remove Twitter username handles from text.<br>\n",
    "* reduce_len=False\n",
    "\n",
    "Returns<br>\n",
    "a tokenized list of strings; concatenating this list returns the original string if preserve_case=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map tweet to it's tokens\n",
    "tweets_text_tokens = {}\n",
    "raw_token_count = 0\n",
    "\n",
    "for tweet_id,tweet_text in tweets_text_flat.items():\n",
    "    #print(tweet_text)\n",
    "    #print(type(tweet_text[0]))\n",
    "    \n",
    "    tweetSplitter = TweetTokenizer(strip_handles=True, reduce_len=False) # removes the @tweet_handle\n",
    "    tweetTokens = tweetSplitter.tokenize(tweet_text[0])\n",
    "    raw_token_count = raw_token_count+len(tweetTokens)\n",
    "    \n",
    "    tweets_text_tokens[tweet_id] = tweetTokens\n",
    "    #break\n",
    "#tweets_text_tokens\n",
    "logger.info(f'Number of Tweet Documents: {len(tweets_text_tokens)}')\n",
    "logger.info(f'Number of Raw Tokens: {raw_token_count}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### INIT FEATURE BoW Count\n",
    "Perform initial Bag Of Words Count - save off for reference and insights into vocabular size reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_features(dic,save_as):\n",
    "    bow = []\n",
    "    # collect kept feature set after cleaning - and count frequencey\n",
    "    kept_features = {}\n",
    "    for tw_id,features in dic.items():\n",
    "        for word in features:\n",
    "            bow.append(word)\n",
    "            if not word in kept_features:\n",
    "                kept_features[word] = 1\n",
    "            else:\n",
    "                word_count = kept_features[word]\n",
    "                kept_features[word] = word_count+1\n",
    "\n",
    "    # put the feature word counts into named dictionary and data frame for simpler sorting and observation\n",
    "    kept_features_named = {'feature':[],'feature_count':[]}\n",
    "    for feature, count in kept_features.items():\n",
    "        kept_features_named['feature'].append(feature)\n",
    "        kept_features_named['feature_count'].append(count)\n",
    "\n",
    "    # convert dictionary to dataframe for easier sorting\n",
    "    kept_features_df = pd.DataFrame(kept_features_named)\n",
    "    kept_features_df_sorted = kept_features_df.sort_values(by=['feature_count','feature'],ascending=False)\n",
    "\n",
    "    # save df as new data source\n",
    "    #save_as = f'{dataDir}/tweets_kept_feature_counts.csv'\n",
    "    kept_features_df_sorted.to_csv(save_as,index=False)\n",
    "\n",
    "    #kept_features_df_sorted.head()\n",
    "    \n",
    "    return kept_features_df_sorted,bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_as = f'{dataDir}/tweets_init_feature_counts.csv'\n",
    "iF = count_features(tweets_text_tokens,save_as=save_as)\n",
    "bag_of_words = iF[1]\n",
    "logger.info(f'Initial Bag Of Word Feature Count: {len(bag_of_words)}')\n",
    "iF[0].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at initial word cloud of bag of words\n",
    "wordcloud_draw(bag_of_words, color='white', max_words=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Cleaning Vocabular Size Reduction* <br>\n",
    "* Total Tokens Prior To Cleaning: 41671\n",
    "* Total Tokens After Cleaning: 22262"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse out hashtags, urls, @xxx, numbers, punctuation from the tweets tokens\n",
    "def clean_tweet_text(tweets_dic,\n",
    "                     custom_stop_words=[],\n",
    "                     remove_pun=True,\n",
    "                     remove_non_alphabetic=True,\n",
    "                     remove_stop_words=True,\n",
    "                     lower_case=False,\n",
    "                     stemming=False,\n",
    "                    ):\n",
    "    tweets_text_hash = {}\n",
    "    tweets_text_url = {}\n",
    "    #tweets_text__tokens_cleaned = {}\n",
    "    total_tokens_prior = 0\n",
    "    total_tokens_after = 0\n",
    "\n",
    "    regex_hash=re.compile('^#.+')\n",
    "    regex_url=re.compile('^http*')\n",
    "\n",
    "\n",
    "    for tweet_id, tokens in tweets_dic.items():\n",
    "        tweet_hashes = []\n",
    "        tweet_urls = []\n",
    "        tweet_numbers = []\n",
    "        tweet_non_words = []\n",
    "        logger.debug(f'tweet: {tweet_id} | feature length prior to text cleaning steps: {len(tokens)}')\n",
    "\n",
    "        total_tokens_prior = total_tokens_prior+len(tweets_dic[tweet_id])\n",
    "        #logger.info(f'Total Tokens Prior To Cleaning: {total_tokens_prior}')\n",
    "        try:\n",
    "            for t in tokens:\n",
    "                if((re.match(regex_hash,t))):\n",
    "                    tweet_hashes.append(t)\n",
    "\n",
    "                elif((re.match(regex_url,t))):\n",
    "                    tweet_urls.append(t)\n",
    "\n",
    "            # remove hash tags\n",
    "            if len(tweet_hashes) > 0:\n",
    "                # remove these hash tokens from tweets_text_tokens\n",
    "                cleaned_tweets_text_tokens = [x for x in tokens if (x not in tweet_hashes)]\n",
    "                tweets_dic[tweet_id] = cleaned_tweets_text_tokens\n",
    "                tokens = tweets_dic[tweet_id]\n",
    "                logger.debug(f'tweet: {tweet_id} | After hash tag removal: {len(tokens)}')\n",
    "\n",
    "            # remove urls\n",
    "            if len(tweet_urls) > 0:\n",
    "                cleaned_tweets_text_tokens = [x for x in tokens if (x not in tweet_urls)]\n",
    "                tweets_dic[tweet_id] = cleaned_tweets_text_tokens\n",
    "                tokens = tweets_dic[tweet_id]\n",
    "                logger.debug(f'tweet: {tweet_id} | After URL removal: {len(tokens)}')\n",
    "\n",
    "            # remove punctuation\n",
    "            if remove_pun:\n",
    "                table = str.maketrans('','',string.punctuation)\n",
    "                stripped = [w.translate(table) for w in tokens]\n",
    "                if len(stripped) > 0:\n",
    "                    tweets_dic[tweet_id] = stripped\n",
    "                    tokens = tweets_dic[tweet_id]\n",
    "                    logger.debug(f'tweet: {tweet_id} | After punctuation removal: {len(tokens)}')\n",
    "\n",
    "            # remove tokens that are not in alphabetic\n",
    "            if remove_non_alphabetic:\n",
    "                alpha_words = [word for word in tokens if word.isalpha()]\n",
    "                if len(alpha_words) > 0:\n",
    "                    tweets_dic[tweet_id] = alpha_words\n",
    "                    tokens = tweets_dic[tweet_id]\n",
    "                    logger.debug(f'tweet: {tweet_id} | After non alphabetic removal: {len(tokens)}')\n",
    "            \n",
    "            # lower case\n",
    "            if lower_case:\n",
    "                lower_words = [word.lower() for word in tokens]\n",
    "                tweets_dic[tweet_id] = lower_words\n",
    "                tokens = tweets_dic[tweet_id]\n",
    "                #logger.debug(f'tweet: {tweet_id} | After lower case: {len(tokens)}')\n",
    "\n",
    "            \n",
    "            # filter out stop words\n",
    "            if remove_stop_words:\n",
    "                stop_words = set(stopwords.words('english'))\n",
    "                new_list = set(list(stop_words) + custom_stop_words)\n",
    "                not_stop_words = [w for w in tokens if not w in stop_words]\n",
    "                if len(not_stop_words) > 0:\n",
    "                    tweets_dic[tweet_id] = not_stop_words\n",
    "                    tokens = tweets_dic[tweet_id]\n",
    "                    logger.debug(f'tweet: {tweet_id} | After stop word removal: {len(tokens)}')\n",
    "            \n",
    "            # consider stemming...???\n",
    "            if stemming:\n",
    "                ps = PorterStemmer()\n",
    "                stem_words = [ps.stem(word) for word in tokens]\n",
    "                tweets_dic[tweet_id] = stem_words\n",
    "                tokens = tweets_dic[tweet_id]\n",
    "                logger.debug(f'tweet: {tweet_id} | After stemming: {len(tokens)}')\n",
    "            \n",
    "            # count tokens\n",
    "            total_tokens_after = total_tokens_after+len(tweets_dic[tweet_id])\n",
    "            \n",
    "        except BaseException as be:\n",
    "            logger.warning(f'**WARNING** Caught BaseException: {be}')\n",
    "            pass\n",
    "\n",
    "    logger.info(f'Total Tokens Prior To Cleaning: {total_tokens_prior}')\n",
    "    logger.info(f'Total Tokens After Cleaning: {total_tokens_after}')\n",
    "    \n",
    "    \n",
    "    return tweets_dic,tweets_text_hash,tweets_text_url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KEPT FEATURES BoW Count\n",
    "Perform word frequency count for kept feature list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tw_clean = clean_tweet_text(tweets_text_tokens,\n",
    "                     custom_stop_words=[],\n",
    "                     remove_pun=True,\n",
    "                     remove_non_alphabetic=True,\n",
    "                     remove_stop_words=True,\n",
    "                     lower_case=True,\n",
    "                     stemming=False,\n",
    "                    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tw_kept_feats = tw_clean[0]\n",
    "tw_hash = tw_clean[1]\n",
    "tw_url = tw_clean[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save tweets as csv\n",
    "# put the feature word counts into named dictionary and data frame for simpler sorting and observation\n",
    "kept_tweet_features = {'tweet_id':[],'features':[]}\n",
    "for tweet_id, features in tw_kept_feats.items():\n",
    "    kept_tweet_features['tweet_id'].append(tweet_id)\n",
    "    kept_tweet_features['features'].append(features)\n",
    "\n",
    "# convert dictionary to dataframe for easier sorting\n",
    "kept_tweet_features_df = pd.DataFrame(kept_tweet_features)\n",
    "kept_tweet_features_df_sorted = kept_tweet_features_df.sort_values(by=['tweet_id'],ascending=True)\n",
    "\n",
    "# save df as new data source\n",
    "save_as = f'{dataDir}/tweets_kept_features.csv'\n",
    "kept_tweet_features_df_sorted.to_csv(save_as,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "save_as = f'{dataDir}/tweets_kept_lower_feature_counts.csv'\n",
    "kf = count_features(tw_kept_feats,save_as=save_as)\n",
    "clean_bag_of_words = kf[1]\n",
    "kf[0].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean_bag_of_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at cleaned word cloud of bag of words\n",
    "wordcloud_draw(clean_bag_of_words, color='white', max_words=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get tweet feature files\n",
    "#kept_feats_file = 'tweets_kept_feature_counts.csv'\n",
    "kept_feats_file = 'tweets_kept_lower_feature_counts.csv'\n",
    "#kept_feats_file = 'tweets_kept_lower_stem_feature_counts.csv'\n",
    "kept_tweet_feats_file = 'tweets_kept_features.csv'\n",
    "\n",
    "kept_feats_counts = pd.read_csv(f'{dataDir}/{kept_feats_file}',error_bad_lines=False, encoding = \"ISO-8859-1\")\n",
    "kept_tweet_feats = pd.read_csv(f'{dataDir}/{kept_tweet_feats_file}',error_bad_lines=False, encoding = \"ISO-8859-1\")\n",
    "#kept_feats_counts.head()\n",
    "kept_tweet_feats.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save off each tweet as it's own file\n",
    "import io\n",
    "for row in kept_tweet_feats.iterrows():\n",
    "    tweet_id = row[1]['tweet_id']\n",
    "    features = row[1]['features']\n",
    "    features = features.replace('[','')\n",
    "    features = features.replace(']','')\n",
    "    features = features.replace('\\'','')\n",
    "    features = features.replace(',','')\n",
    "    \n",
    "    with io.open(f'{corpusDir}/{tweet_id}_ai_tweet_text.txt','w+',encoding='utf8') as f:\n",
    "        f.write(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Vectorization Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create integer feature vector mappings\n",
    "feature_id_map = {}\n",
    "id_feature_map = {}\n",
    "\n",
    "feats = kept_feats_counts.feature\n",
    "\n",
    "for i,f in enumerate(feats):\n",
    "    id_feature_map[i] = f\n",
    "    feature_id_map[f] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from time import time\n",
    "import shorttext\n",
    "#from keras.preprocessing.text import Tokenizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[sklearn CountVectorizer](https://scikit-learn.org/0.15/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer)<br>\n",
    "Convert a collection of text documents to a matrix of token counts<br>\n",
    "\n",
    "This implementation produces a sparse representation of the counts using scipy.sparse.coo_matrix.<br>\n",
    "\n",
    "If you do not provide an a-priori dictionary and you do not use an analyzer that does some kind of feature selection then the number of features will be equal to the vocabulary size found by analyzing the data<br>\n",
    "\n",
    "In text mining, it is important to create the document-term matrix (DTM) of the corpus we are interested in. A DTM is basically a matrix, with documents designated by rows and words by columns, that the elements are the counts or the weights (usually by tf-idf). Subsequent analysis is usually based creatively on DTM.<br>\n",
    "\n",
    "CountVectorizer supports counts of N-grams of words or consecutive characters. Once fitted, the vectorizer has built a dictionary of feature indices:<br>\n",
    "The index value of a word in the vocabulary is linked to its frequency in the whole training corpus.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get tweet files from path\n",
    "tweet_files = []\n",
    "tweet_filenames = {}\n",
    "\n",
    "path=f'{corpusDir}/'\n",
    "for dirpath, dirs, files in os.walk(path):\n",
    "    #print(dirpath)\n",
    "    #print(dirs)\n",
    "    #print(files)\n",
    "    for i,f in enumerate(files):\n",
    "        tweet_files.append(dirpath+f)\n",
    "        tweet_filenames[i] = f\n",
    "#tweet_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_files[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test CountVectorizer from sklearn\n",
    "count_vectorizer = CountVectorizer(input='filename',\n",
    "                                   #ngram_range=(1,1),\n",
    "                                   #max_df=1.0,\n",
    "                                   #min_df=0.001,\n",
    "                                   stop_words='english',\n",
    "                                   max_features=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Returns:\t\n",
    "#X : array, [n_samples, n_features]\n",
    "fit_vec = count_vectorizer.fit(tweet_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#logger.info(f'CountVectorizer Fit: \\n{fit_vec.vocabulary_}')\n",
    "voc_dict = dict(fit_vec.vocabulary_)\n",
    "voc_df = pd.DataFrame.from_dict(voc_dict, orient='index').reset_index()\n",
    "voc_df.columns=('feature','feature_count')\n",
    "voc_df.sort_values(by='feature_count', ascending=False)[::10].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voc_df.feature_count.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdm_vec = fit_vec.transform(tweet_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(f'CountVectorizer TDM shape:{tdm_vec.shape}')\n",
    "logger.info(f'CountVectorizer TDM size:{tdm_vec.size}')\n",
    "logger.info(f'CountVectorizer TDM type:{type(tdm_vec)}')\n",
    "logger.info(f'CountVectorizer TDM toarray:{type(tdm_vec.toarray())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_id_map = {}\n",
    "id_word_map = {}\n",
    "\n",
    "words = fit_vec.get_feature_names()\n",
    "logger.debug(words)\n",
    "\n",
    "for i,f in enumerate(words):\n",
    "    word_id_map[i] = f\n",
    "    id_word_map[f] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = fit_vec.get_feature_names()\n",
    "tdm_vec_df = pd.DataFrame(tdm_vec.toarray(),columns=cols)\n",
    "non_zero_field_count = 0\n",
    "# output feature vector term frequence vector as 'doc feature frequency'\n",
    "with open(f'{dataDir}/feature_vector_tf.txt','w+') as f:\n",
    "\n",
    "    for i in range(0,tdm_vec_df.shape[0]):\n",
    "        a = [index for index,value in enumerate(tdm_vec_df.iloc[i]) if value > 0]\n",
    "        sent = tweet_filenames[i]\n",
    "        non_zero_field_count = non_zero_field_count+len(a)\n",
    "        \n",
    "        for col in a:\n",
    "            v = tdm_vec_df.iloc[i,col]\n",
    "            sent = sent+' '+word_id_map[col]+' '+str(v)\n",
    "        \n",
    "        f.write(sent+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdm_vec_df[::100].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(2649*4721)*4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = [558, 1699, 1913, 2457, 2572, 3411, 3938]\n",
    "for i in test:\n",
    "    print(word_id_map[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[sklearn TfidfVectorizer](https://scikit-learn.org/0.15/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer)<br>\n",
    "\n",
    "To avoid these potential discrepancies it suffices to divide the number of occurrences of each word in a document by the total number of words in the document: these new features are called tf for Term Frequencies.<br>\n",
    "\n",
    "Another refinement on top of tf is to downscale weights for words that occur in many documents in the corpus and are therefore less informative than those that occur only in a smaller portion of the corpus.<br>\n",
    "\n",
    "This downscaling is called tf–idf for “Term Frequency times Inverse Document Frequency”.<br>\n",
    "\n",
    "Both tf and tf–idf can be computed as follows using TfidfTransformer:<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test TfidfVectorizer from sklearn - \n",
    "tfidf_vectorizer = TfidfVectorizer(input='filename',smooth_idf=False, sublinear_tf=False, norm=None, analyzer='word')\n",
    "idf_vec = tfidf_vectorizer.fit(tweet_files)\n",
    "tfidf_vec = idf_vec.transform(tweet_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf_feat_names = idf_vec.get_feature_names()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#idf_vec.vocabulary_\n",
    "idf = tfidf_vectorizer.idf_\n",
    "idf_weights = dict(zip(idf_vec.get_feature_names(), idf))\n",
    "logger.info(idf_weights)\n",
    "#idf_weights_df = pd.DataFrame(idf_weights)\n",
    "#idf_weights_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph inverse document frequency\n",
    "token_weight = pd.DataFrame.from_dict(idf_weights, orient='index').reset_index()\n",
    "token_weight.columns=('token','weight')\n",
    "token_weight = token_weight.sort_values(by='weight', ascending=False)\n",
    "token_weight.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#graph inverse document frequency\n",
    "sns.barplot(x='token', y='weight', data=token_weight)            \n",
    "plt.title(\"Inverse Document Frequency(idf) per token\")\n",
    "fig=plt.gcf()\n",
    "fig.set_size_inches(10,5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing inverse document frequency\n",
    "# get feature names\n",
    "feature_names = np.array(tfidf_vectorizer.get_feature_names())\n",
    "sorted_by_idf = np.argsort(tfidf_vectorizer.idf_)\n",
    "logger.info(\"Features with lowest idf:\\n{}\".format(\n",
    "       feature_names[sorted_by_idf[:3]]))\n",
    "logger.info(\"\\nFeatures with highest idf:\\n{}\".format(\n",
    "       feature_names[sorted_by_idf[-3:]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Weight of tokens per document**<br>\n",
    "the more times a token appears in a document, the more weight it will have. However, the more documents the token appears in, it is 'penalized' and the weight is diminished. For example, the weight for token 'not' is 4, but if it did not appear in all documents (that is, only in one document) its weight would have been 8.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The token 'not' has  the largest weight in document #2 because it appears 3 times there. But in document #1\\\n",
    " its weight is 0 because it does not appear there.\")\n",
    "tfidf_vec.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TF-IDF - Maximum token value throughout the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_tf = idf_vec.transform(tweet_files)\n",
    "\n",
    "# find maximum value for each of the features over all of dataset:\n",
    "max_val = new_tf.max(axis=0).toarray().ravel()\n",
    "\n",
    "#sort weights from smallest to biggest and extract their indices \n",
    "sort_by_tfidf = max_val.argsort()\n",
    "\n",
    "print(\"Features with lowest tfidf:\\n{}\".format(\n",
    "      feature_names[sort_by_tfidf[:3]]))\n",
    "\n",
    "print(\"\\nFeatures with highest tfidf: \\n{}\".format(\n",
    "      feature_names[sort_by_tfidf[-3:]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
