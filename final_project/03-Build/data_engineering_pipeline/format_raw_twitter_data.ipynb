{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Ryan Timbrook (RTIMBROO)  \n",
    "DATE: 12/3/2019 <br>\n",
    "Topic: \n",
    "\n",
    "## 1. Objective:\n",
    "-----------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as nb\n",
    "import json\n",
    "import os\n",
    "from os import path\n",
    "import fnmatch\n",
    "import io\n",
    "import re\n",
    "import string\n",
    "from datetime import date\n",
    "from datetime import time\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custome python packages\n",
    "import rtimbroo_utils as br  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set global properties\n",
    "notebook_file_name = 'format_raw_twitter_data'\n",
    "report_file_name = 'format_raw_twitter_data'\n",
    "app_name = 'format_raw_twitter_data'\n",
    "log_level = 10 # 10-DEBUG, 20-INFO, 30-WARNING, 40-ERROR, 50-CRITICAL\n",
    "\n",
    "# setup working directory structure\n",
    "# set global properties\n",
    "dataDir = './data'\n",
    "outputDir = './output'\n",
    "configDir = './config'\n",
    "logOutDir = './logs'\n",
    "imageDir = './images'\n",
    "modelDir = './models'\n",
    "corpusDir = './corpus'\n",
    "# create base output directories if they don't exist\n",
    "if not os.path.exists(outputDir): os.mkdir(outputDir)\n",
    "if not os.path.exists(logOutDir): os.mkdir(logOutDir)\n",
    "if not os.path.exists(imageDir): os.mkdir(imageDir)\n",
    "if not os.path.exists(modelDir): os.mkdir(modelDir)\n",
    "if not os.path.exists(dataDir): os.mkdir(dataDir)\n",
    "if not os.path.exists(configDir): os.mkdir(configDir)\n",
    "if not os.path.exists(corpusDir): os.mkdir(corpusDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a logger for troubleshooting / data exploration\n",
    "logger = br.getFileLogger(logOutDir+'/',app_name,level=log_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set what to search on\n",
    "nfl_type = 'player'\n",
    "search_on = 'deshaun_watson'\n",
    "# setup base twitter search query\n",
    "search_terms=\"deshaun watson\"\n",
    "\n",
    "rawDataDir = \"player/deshaun_watson\"\n",
    "tw_path = f'{dataDir}/{rawDataDir}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# walk the directory structure pulling in json raw data to be parsed\n",
    "# function to get the directory path location of corpus files to vectorize\n",
    "def get_raw_tweets_by_version(path,version):\n",
    "    _files = []\n",
    "    \n",
    "    path = f'{path}/v{version}'\n",
    "    \n",
    "    for dirpath, dirs, files in os.walk(path):\n",
    "        logger.debug(f'{dirpath}')\n",
    "        logger.debug(f'{dirs}')\n",
    "        logger.debug(f'{files}')\n",
    "        \n",
    "        for d in dirs:\n",
    "            for file in os.listdir(f'{dirpath}/{d}'):\n",
    "                logger.debug(f'{dirpath}/{d}: files: {file}')\n",
    "                if 'raw' in file:\n",
    "                    _files.append(f'{dirpath}/{d}/{file}')\n",
    "        break\n",
    "    logger.info(f'_files: {_files}')\n",
    "            \n",
    "    logger.info(f'version:{version}, _files collected: {len(_files)}')\n",
    "    \n",
    "    \n",
    "    return _files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweetFiles = get_raw_tweets_by_version(tw_path,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_raw_tweet(result):\n",
    "# search tweets\n",
    "    tweets_dict = {'id':[],'created_at':[],'date':[],'time':[],'user':[],'text':[],'favorite_count':[], 'year':[], 'month':[], 'day_of_month':[], 'day_of_week':[]}\n",
    "    tweets_text_metadata_dict = {'id':[],'date':[],'user':[],'urls':[],'hash_tags':[],'at_tags':[]}\n",
    "   \n",
    "    try: \n",
    "        \n",
    "        try:\n",
    "            logger.debug(f'{result[\"id_str\"]} | {result[\"user\"][\"screen_name\"]} | {result[\"created_at\"]} | {result[\"text\"]} | {result[\"user\"][\"favourites_count\"]}')\n",
    "        except BaseException as be:\n",
    "            logger.warning(f'page_search: ***WARNING***: Caught BaseException writing debug log file: {be}')\n",
    "\n",
    "        # if tweet_mode='extended', use _result['full_text']\n",
    "        text = ''\n",
    "        try:\n",
    "            text = result['retweeted_status'][\"extended_tweet\"]['full_text']\n",
    "        except BaseException as be:\n",
    "            logger.warning(f'page_search: NO full_text: {be}')\n",
    "            text = result['text']\n",
    "\n",
    "        # add key attributes to tweets dictionary as return results\n",
    "        tweets_dict['id'].append(result[\"id_str\"])\n",
    "        tweets_dict['created_at'].append(result[\"created_at\"])\n",
    "        tweets_dict['favorite_count'].append(result[\"user\"][\"favourites_count\"])\n",
    "\n",
    "        # call function to parse string date\n",
    "        date_time = br.convert_str_date(result[\"created_at\"]) # get datetime components\n",
    "\n",
    "        tweets_dict['date'].append(date_time[0])\n",
    "        tweets_dict['time'].append(date_time[1])        \n",
    "        tweets_dict['user'].append(result[\"user\"][\"screen_name\"])\n",
    "\n",
    "        # call function to parse text for metadata\n",
    "        clean_text = br.clean_tweet_text_meta(logger, text)\n",
    "\n",
    "        tweets_dict['text'].append(clean_text[0])\n",
    "\n",
    "        # create dictionary of tweet text metadata\n",
    "        tweets_text_metadata_dict['id'].append(result[\"id_str\"])\n",
    "        tweets_text_metadata_dict['date'].append(date_time[0])\n",
    "        tweets_text_metadata_dict['user'].append(result[\"user\"][\"screen_name\"])\n",
    "        tweets_text_metadata_dict['urls'].append(clean_text[1])\n",
    "        tweets_text_metadata_dict['hash_tags'].append(clean_text[2])\n",
    "        tweets_text_metadata_dict['at_tags'].append(clean_text[3])\n",
    "\n",
    "        # track timeseries attributes for granular reporting and visualizations\n",
    "        tweets_dict['year'].append(date_time[2])\n",
    "        tweets_dict['month'].append(date_time[3])\n",
    "        tweets_dict['day_of_month'].append(date_time[4])\n",
    "        tweets_dict['day_of_week'].append(date_time[5])\n",
    "\n",
    "    except BaseException as be:\n",
    "        logger.warning(f'**WARNING** Caught BaseException: {be}')\n",
    "    \n",
    "    logger.info(f'process_raw_tweet: completed processing...')\n",
    "    \n",
    "    return pd.DataFrame.from_dict(tweets_dict), pd.DataFrame.from_dict(tweets_text_metadata_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "search_range_results_df = pd.DataFrame()\n",
    "search_tweets_text_meta_df = pd.DataFrame()\n",
    "tweet_cnt = 0\n",
    "result_df = None\n",
    "\n",
    "for tweet_file in tweetFiles:\n",
    "    with io.open(f'{tweet_file}', 'r',encoding='utf8') as f:\n",
    "        for tweet in f.readlines():\n",
    "            tweet_cnt +=1\n",
    "            logger.info(f'reading tweet: {tweet_cnt}')\n",
    "            logger.debug(f'tweet {tweet_cnt}:\\n {tweet}')\n",
    "            \n",
    "            # convert string dictionary to dictionary\n",
    "            tweet = json.loads(tweet)\n",
    "            \n",
    "            \n",
    "            result_df = process_raw_tweet(tweet)\n",
    "            \n",
    "            # merge dataframes - complete table of search results collected and written out to csv file in code block below\n",
    "            search_range_results_df = search_range_results_df.append(result_df[0], ignore_index=True)\n",
    "            search_tweets_text_meta_df = search_tweets_text_meta_df.append(result_df[1], ignore_index=True)\n",
    "            \n",
    "            #break\n",
    "    #break\n",
    "    \n",
    "\n",
    "logger.info(f'search_range_results_df shape: {search_range_results_df.shape} | head:\\n{search_range_results_df.head()}')\n",
    "logger.debug(f'{search_tweets_text_meta_df.head()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SAVE DATA FRAME of TWEET TIMESERIES TEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputPath = f'{dataDir}/{nfl_type}/{search_on}/v1'\n",
    "search_range_results_df.to_csv(f'{outputPath}/search_result_tweet_text_data.csv', index=False)\n",
    "search_tweets_text_meta_df.to_csv(f'{outputPath}/search_result_tweet_text_meta.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
