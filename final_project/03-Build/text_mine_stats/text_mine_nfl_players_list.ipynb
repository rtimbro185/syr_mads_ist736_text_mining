{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IST 736 Text Mining\n",
    "### Web Scrap NFL Active Players List\n",
    "\n",
    "\n",
    "Ryan Timbrook (RTIMBROO)  \n",
    "DATE:10/18/2019<br>\n",
    "Topic: <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Learning Objective\n",
    "_____________________________________________________________________________________________\n",
    "In this assignment, you have the freedom to find an existing text corpus, or create a new text corpus of your interest. If you are creating a new corpus, make sure it is not too large and thus too time-consuming to create.  \n",
    " \n",
    "Then you will vectorize the text corpus using any tool that you are comfortable with: Weka, R, Python, etc. Explain the decisions you made during the vectorization process, e.g., did you merge lower- and uppercase? Then you will explore the text vectors and see if you can find anything interesting. The lectures showed some examples of comparative analysis and trend analysis. But you have the freedom to define what would be interesting patterns as long as you can explain it in a sensible way.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# toggle for working with colab\n",
    "isColab = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#*ONLY RUN WHEN WORKING ON COLAB*\n",
    "#===================================================\n",
    "# mount google drive for working in colab\n",
    "\n",
    "#from google.colab import drive\n",
    "#drive.mount('/content/gdrive', force_remount=True)\n",
    "\n",
    "# working within colab, set base working directory\n",
    "#base_dir = \"./gdrive/My Drive/IST707_PRJ_Realestate/buy_rent_sell/\"\n",
    "\n",
    "# validate directory mapping\n",
    "#ls f'{base_dir}'\n",
    "\n",
    "# upload custome python files\n",
    "#from google.colab import files\n",
    "#uploaded_files = files.upload()\n",
    "\n",
    "# print files uploaded\n",
    "#for f in uploaded_files.keys():\n",
    "#  print(f'file name: {f}')\n",
    "\n",
    "#isColab = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______________________________________________________________________________________________\n",
    "### Coding Environment Setup\n",
    "Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages for analysis and modeling\n",
    "import pandas as pd #data frame operations\n",
    "import numpy as np #arrays and math functions\n",
    "import matplotlib.pyplot as plt #2D plotting\n",
    "%matplotlib inline\n",
    "import seaborn as sns #\n",
    "import requests\n",
    "import os\n",
    "import io\n",
    "import string\n",
    "import urllib\n",
    "from bs4 import BeautifulSoup\n",
    "import pprint\n",
    "import lxml.html\n",
    "from datetime import date\n",
    "from datetime import time\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# packages for twitter\n",
    "import tweepy as tw\n",
    "import codecs\n",
    "from tweepy import OAuthHandler\n",
    "import json\n",
    "from tweepy import Stream\n",
    "from tweepy.streaming import StreamListener\n",
    "\n",
    "# packages for NLTK\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "import re\n",
    "\n",
    "import sys\n",
    "from os import path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk downloads\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custome python packages\n",
    "import rtimbroo_utils as br             # custome python helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set global properties\n",
    "notebook_file_name = 'text_mine_nfl_players_list'\n",
    "report_file_name = 'Text Mine Web Crawl'\n",
    "app_name = 'Web Crawl NFL Players List'\n",
    "log_level = 10 # 10-DEBUG, 20-INFO, 30-WARNING, 40-ERROR, 50-CRITICAL\n",
    "\n",
    "# setup working directory structure\n",
    "# set global properties\n",
    "if not isColab:\n",
    "    dataDir = './data'\n",
    "    outputDir = './output'\n",
    "    configDir = './config'\n",
    "    logOutDir = './logs'\n",
    "    imageDir = './images'\n",
    "    modelDir = './models'\n",
    "    corpusDir = './corpus'\n",
    "else:\n",
    "    # working within colab\n",
    "    dataDir = f'{base_dir}data'\n",
    "    outputDir = f'{base_dir}output'\n",
    "    configDir = f'{base_dir}config'\n",
    "    logOutDir = f'{base_dir}logs'\n",
    "    imageDir = f'{base_dir}images'\n",
    "    modelDir = f'{base_dir}models'\n",
    "    corpusDir = f'{base_dir}corpus'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create base output directories if they don't exist\n",
    "if not os.path.exists(outputDir): os.mkdir(outputDir)\n",
    "if not os.path.exists(logOutDir): os.mkdir(logOutDir)\n",
    "if not os.path.exists(imageDir): os.mkdir(imageDir)\n",
    "if not os.path.exists(modelDir): os.mkdir(modelDir)\n",
    "if not os.path.exists(dataDir): os.mkdir(dataDir)\n",
    "if not os.path.exists(configDir): os.mkdir(configDir)\n",
    "if not os.path.exists(corpusDir): os.mkdir(corpusDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a logger for troubleshooting / data exploration\n",
    "logger = br.getFileLogger(logOutDir+'/',app_name,level=log_level)\n",
    "np.random.seed(42) # NumPy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile Classes and Local Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Team(object):\n",
    "    \n",
    "    def __init__(self,team_name):\n",
    "        self.team_name = team_name\n",
    "        self.roster_year = ''\n",
    "        self.roster_players = []\n",
    "        self.team_roster_player_stats = {}\n",
    "        self.team_text = {}\n",
    "        self.player_text = {}\n",
    "            \n",
    "    def set_roster_year(self,year):\n",
    "        self.roster_year = year\n",
    "        \n",
    "    def set_roster_players(self,player):\n",
    "        self.roster_players.append(player)\n",
    "        \n",
    "    def set_team_text(self,source,text_topic,text):\n",
    "        now = datetime.utcnow().isoformat()\n",
    "        logger.info(f'set_team_text: source:[{source}] | text_topic: [{text_topic}] | text: {text}')\n",
    "        \n",
    "        if source in self.team_text.keys():\n",
    "            logger.info(f'source: {source} is in team_tex: {self.team_text}')\n",
    "            \n",
    "            if text_topic in self.team_text[source].keys():\n",
    "                logger.info(f'text_topic: {text_topic} is in team_text: {self.team_text}')\n",
    "                self.team_text[source][text_topic].update({now:text})\n",
    "            else:\n",
    "                logger.info(f'text_topic: {text_topic} not in team_text: {self.team_text}')\n",
    "                self.team_text[source][text_topic] = {now:text}\n",
    "        else:\n",
    "            logger.info(f'source: {source} not in team_text: {self.team_text}')\n",
    "            self.team_text[source] = {text_topic:{now:text}}\n",
    "        \n",
    "    def set_player_text(self,text_topic,text):\n",
    "        now = datetime.utcnow().isoformat()\n",
    "        if text_topic in self.player_text:\n",
    "            self.player_text[text_topic].update({now:text})\n",
    "        else:\n",
    "            self.player_text[text_topic] = {now:text}\n",
    "            \n",
    "    def set_team_roster_players_stats(self,stats):\n",
    "        now = datetime.utcnow().isoformat()\n",
    "        key = re.findall('^[0-9]{4}-[0-9]{2}-[0-9]{2}',now)\n",
    "        if key in self.team_roster_player_stats:\n",
    "            self.team_roster_player_stats[key].update({key:text})\n",
    "        else:\n",
    "            self.team_roster_player_stats[key] = {key:text}\n",
    "    \n",
    "    def get_team_name(self):\n",
    "        return self.team_name\n",
    "    \n",
    "    def get_roster_year(self):\n",
    "        return self.roster_year\n",
    "    \n",
    "    def get_roster_players(self):\n",
    "        return self.roster_players\n",
    "    \n",
    "    def get_team_text_by_topic(self,topic):\n",
    "        team_text = []\n",
    "        logger.info(f'get_team_text_by_topic: topic:[{topic}]')\n",
    "        for s,t in self.team_text.items():\n",
    "            logger.info(f'get_team_text_by_topic: key:[{s}] | value:[{t}]')\n",
    "            if topic in self.team_text[s].keys():\n",
    "                logger.info(f'get_team_text_by_topic: topic:[{s}] is in {self.team_text[s].keys()}' )\n",
    "                team_text.append(self.team_text[s])\n",
    "        \n",
    "        return team_text\n",
    "    \n",
    "    def get_team_text_by_source(self,source):\n",
    "        return self.team_text[source]\n",
    "    \n",
    "    def get_player_text_by_topic(self,topic):\n",
    "        return self.player_text[topic]\n",
    "    \n",
    "    def get_team_roster_players_stats(self):\n",
    "        return self.team_roster_player_stats\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# team \n",
    "def create_team(parser):\n",
    "    # get team name and year\n",
    "    page_title = parser.title.string.strip()\n",
    "    team = page_title.replace('Roster','')\n",
    "    team = team.strip()\n",
    "    year = re.findall('^\\d+',team)\n",
    "    year = year[0]\n",
    "    team = team.replace(year,'').lstrip().lower()\n",
    "    team = team.replace(' ','_')\n",
    "    team = team.replace('-','_')\n",
    "    \n",
    "    logger.info(f'[{team}]')\n",
    "    \n",
    "    # instantiate the nfl team object\n",
    "    new_team = Team(team)\n",
    "    new_team.set_roster_year(year)\n",
    "    \n",
    "    return new_team"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get team text - anything on the roster page\n",
    "# scrape team text\n",
    "def get_team_text(parser):\n",
    "    \n",
    "    page_text = parser.find_all('p')\n",
    "    page_text = lxml.html.fromstring(str(page_text)).text_content()\n",
    "    #logger.info(page_text)\n",
    "    return page_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# player search\n",
    "def player_search(parser):\n",
    "    player_name_search = re.compile('^.+(player-name-col-lg).+')\n",
    "    team_players = set()\n",
    "    \n",
    "    # get list of team players\n",
    "    span = parser.find_all('span')\n",
    "    for s in span:\n",
    "        if re.match(player_name_search,str(s)):\n",
    "            name = lxml.html.fromstring(str(s)).text_content()\n",
    "            team_players.add(name)\n",
    "    \n",
    "    return list(team_players)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_str_index(text,index=0,replacement=''):\n",
    "    return '%s%s%s'%(text[:index],replacement,text[index+1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. OBTAIN the data   \n",
    "________________________________________________________________________________________________\n",
    "* Step 1: [Get Active NFL Players List](https://www.lineups.com/nfl/rosters)\n",
    "    * \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(datetime.utcnow().isoformat())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Web Page's to scrape\n",
    "nfl_roster_url = 'https://www.lineups.com/nfl/rosters'\n",
    "nfl_roster_root_url = 'https://www.lineups.com'\n",
    "team_source = 'lineups.com'\n",
    "people_source = 'lineups.com'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = urllib.request.urlopen(nfl_roster_url).read()\n",
    "soup = BeautifulSoup(html,'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(soup.title.string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve all of the anchor tags\n",
    "roster_tags = soup('a')\n",
    "tags = []\n",
    "tag_urls = []\n",
    "tag_content = []\n",
    "tag_attr = []\n",
    "for tag in roster_tags:\n",
    "    # Look at the parts of a tag\n",
    "    tags.append(tag)\n",
    "    tag_urls.append(tag.get('href', None))\n",
    "    tag_content.append(tag.contents[0])\n",
    "    tag_attr.append(tag.attrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a roster dataframe\n",
    "roster_df = pd.DataFrame()\n",
    "roster_df['Tag'] = tags\n",
    "roster_df['URL'] = tag_urls\n",
    "roster_df['Content'] = tag_content\n",
    "roster_df['Attrs'] = tag_attr\n",
    "roster_df.head()\n",
    "#logger.debug(roster_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter urls to just roster '/nfl/roster'\n",
    "#nfl_roster_root_url\n",
    "team_roster_uris = set()\n",
    "pattern = re.compile(r'^(/nfl/roster/).+')\n",
    "\n",
    "for url in roster_df['URL']:\n",
    "    #logger.info(url)\n",
    "    if not url == None:\n",
    "        if re.match(pattern,url):\n",
    "            logger.debug(url)\n",
    "            team_roster_uris.add(nfl_roster_root_url+url)\n",
    "\n",
    "logger.info(f'NFL Team Roster URL Count: {len(team_roster_uris)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HTML Page Search Detail Examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# loop over the Team Roster URLs - searching each of the subpages for the team roster\n",
    "# Capture Title - Page Text about the Team - Player Names\n",
    "#Player Names: <span class=\"player-name-col-lg\">Matt Ryan</span>\n",
    "\n",
    "player_name_search = re.compile('^.+(player-name-col-lg).+')\n",
    "nfl_teams = []\n",
    "team_roster_urls = list(team_roster_uris)\n",
    "\n",
    "for u in team_roster_urls:\n",
    "    nfl_team = None\n",
    "    # get NFL Team Roster HTML Page\n",
    "    logger.info(f'NFL Roster HTML Page to scrape: {u}')\n",
    "    try:\n",
    "        html = urllib.request.urlopen(u).read()\n",
    "        soup = BeautifulSoup(html,'html.parser')\n",
    "\n",
    "        # create a new team object\n",
    "        nfl_team = create_team(soup)\n",
    "        team_text = get_team_text(soup)\n",
    "        \n",
    "        # dump team text - having issues with URLs being found - site is in the process of updating their pages\n",
    "        dumpDir = f'{corpusDir}/dump'\n",
    "        if not os.path.exists(dumpDir): os.makedirs(dumpDir)\n",
    "        with io.open(f'{dumpDir}/{nfl_team.get_team_name()}.txt','w+',encoding='utf8') as f:\n",
    "            f.write(team_text)\n",
    "        \n",
    "        nfl_team.set_team_text(team_source,'team_roster_news',team_text)\n",
    "\n",
    "        # get list of team players\n",
    "        players = player_search(soup)\n",
    "        for player in players:\n",
    "            nfl_team.set_roster_players(player)\n",
    "\n",
    "        nfl_teams.append(nfl_team)\n",
    "        \n",
    "    except BaseException as be:\n",
    "        logger.warning(f'**WARNING** Caught Exception: {be} | URL: {u}')\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(nfl_teams)\n",
    "for team in nfl_teams:\n",
    "    print(team)\n",
    "    print(team.get_team_name())\n",
    "    logger.debug(team.get_team_text_by_topic('team_roster_news'))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the teams corpus\n",
    "teams = []\n",
    "team_years = []\n",
    "team_texts = []\n",
    "team_rosters = []\n",
    "    \n",
    "# load lists for dataframe\n",
    "for t in nfl_teams:\n",
    "    teams.append(t.get_team_name())\n",
    "    team_years.append(t.get_roster_year())\n",
    "    team_texts.append(t.get_team_text_by_source(team_source)) # not working \n",
    "    team_rosters.append(t.get_roster_players()) # not working\n",
    "    \n",
    "    # save each team text to it's own file - creating a corpus\n",
    "    # create a directory for each team under the corpusDir\n",
    "    teamDir = f'{corpusDir}/teams/{team_source}/{t.get_team_name()}'\n",
    "    if not os.path.exists(teamDir): os.makedirs(teamDir)\n",
    "    \n",
    "    for topic,texts in t.get_team_text_by_source(team_source).items():\n",
    "        topicDir = f'{teamDir}/{topic}'\n",
    "        if not os.path.exists(topicDir): os.makedirs(topicDir)\n",
    "        for key,text in texts.items():\n",
    "            k = re.findall('^[0-9]{4}-[0-9]{2}-[0-9]{2}',key)\n",
    "            #print(k[0])\n",
    "            with io.open(f'{topicDir}/_{k[0]}_{t.get_team_name()}_team_text.txt','w+',encoding='utf8') as f:\n",
    "                f.write(text)\n",
    "\n",
    "# create dataframe\n",
    "nfl_team_df = pd.DataFrame()   \n",
    "nfl_team_df['team'] = teams\n",
    "nfl_team_df['year'] = team_years\n",
    "nfl_team_df['roster'] = team_rosters\n",
    "nfl_team_df['text'] = team_texts\n",
    "\n",
    "# save to csv as new datasource\n",
    "save_as = f'{dataDir}/nfl_team_data_scrapped.csv'\n",
    "nfl_team_df.to_csv(save_as,index=False)\n",
    "    \n",
    "    #break\n",
    "nfl_team_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 SCRUB / CLEAN\n",
    "Perform vectorization tasks\n",
    "\n",
    "Goal: Vectorize NFL Team Text data scrapped from web<br>\n",
    "Each Team Text is considered an individual document<br>\n",
    "\n",
    "Determin **what to count** and **how to count it**<br>\n",
    "\n",
    "Basic text preparation pipeline:\n",
    "\n",
    "* Load the raw text.\n",
    "* Split into tokens.\n",
    "* Convert to lowercase. -> not for sentiment analysis\n",
    "* Remove punctuation from each token.\n",
    "* Filter out remaining tokens that are not alphabetic.\n",
    "* Filter out tokens that are stop words.\n",
    "* Perform stemming -> [nltk reference](https://pythonprogramming.net/stemming-nltk-tutorial/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global variables\n",
    "initial_words_count = 0\n",
    "cleaned_words_count = 0\n",
    "feature_thres = 2\n",
    "rare_thres = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordcloud_draw(data, color='black', width=1000, height=750, max_font_size=50, max_words=100):\n",
    "    words = ' '.join([word for word in data])\n",
    "    #cleaned_word = \" \".join([word for word in words])\n",
    "    wordcloud = WordCloud(stopwords=STOPWORDS,\n",
    "                    background_color=color,\n",
    "                    width=width,\n",
    "                    height=height,\n",
    "                    max_font_size=max_font_size,\n",
    "                    max_words=max_words,\n",
    "                     ).generate(words)\n",
    "    plt.figure(1,figsize=(10.5, 7))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_features(dic,save_as):\n",
    "    bow = []\n",
    "    # collect kept feature set after cleaning - and count frequencey\n",
    "    kept_features = {}\n",
    "    for _id,features in dic.items():\n",
    "        for word in features:\n",
    "            bow.append(word)\n",
    "            if not word in kept_features:\n",
    "                kept_features[word] = 1\n",
    "            else:\n",
    "                word_count = kept_features[word]\n",
    "                kept_features[word] = word_count+1\n",
    "\n",
    "    # put the feature word counts into named dictionary and data frame for simpler sorting and observation\n",
    "    kept_features_named = {'feature':[],'feature_count':[]}\n",
    "    for feature, count in kept_features.items():\n",
    "        kept_features_named['feature'].append(feature)\n",
    "        kept_features_named['feature_count'].append(count)\n",
    "\n",
    "    # convert dictionary to dataframe for easier sorting\n",
    "    kept_features_df = pd.DataFrame(kept_features_named)\n",
    "    kept_features_df_sorted = kept_features_df.sort_values(by=['feature_count','feature'],ascending=False)\n",
    "\n",
    "    # save df as new data source\n",
    "    #save_as = f'{dataDir}/kept_feature_counts.csv'\n",
    "    kept_features_df_sorted.to_csv(save_as,index=False)\n",
    "\n",
    "    #kept_features_df_sorted.head()\n",
    "    \n",
    "    return kept_features_df_sorted,bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean text\n",
    "def clean_text(text_dic,\n",
    "                     custom_stop_words=[],\n",
    "                     remove_pun=True,\n",
    "                     remove_non_alphabetic=True,\n",
    "                     remove_stop_words=True,\n",
    "                     lower_case=False,\n",
    "                     stemming=False,\n",
    "                    ):\n",
    "    \n",
    "    total_tokens_prior = 0\n",
    "    total_tokens_after = 0\n",
    "    \n",
    "    regex_hash=re.compile('^#.+')\n",
    "    regex_url=re.compile('^http*')\n",
    "\n",
    "    for _id, tokens in text_dic.items():\n",
    "        hashes = []\n",
    "        urls = []\n",
    "        numbers = []\n",
    "        non_words = []\n",
    "        logger.info(f'text: {_id} | feature length prior to text cleaning steps: {len(tokens)}')\n",
    "\n",
    "        total_tokens_prior = total_tokens_prior+len(text_dic[_id])\n",
    "        logger.info(f'Total Tokens Prior To Cleaning: {total_tokens_prior}')\n",
    "        \n",
    "        try:\n",
    "            for t in tokens:\n",
    "                if((re.match(regex_hash,t))):\n",
    "                    hashes.append(t)\n",
    "\n",
    "                elif((re.match(regex_url,t))):\n",
    "                    urls.append(t)\n",
    "\n",
    "            # remove hash tags\n",
    "            if len(hashes) > 0:\n",
    "                # remove these hash tokens from text_tokens\n",
    "                cleaned_text_tokens = [x for x in tokens if (x not in hashes)]\n",
    "                text_dic[_id] = cleaned_text_tokens\n",
    "                tokens = text_dic[_id]\n",
    "                logger.info(f'text: {_id} | After hash tag removal: {len(tokens)}')\n",
    "\n",
    "            # remove urls\n",
    "            if len(urls) > 0:\n",
    "                cleaned_text_tokens = [x for x in tokens if (x not in urls)]\n",
    "                text_dic[_id] = cleaned_text_tokens\n",
    "                tokens = text_dic[_id]\n",
    "                logger.info(f'text: {_id} | After URL removal: {len(tokens)}')\n",
    "\n",
    "            # remove punctuation\n",
    "            if remove_pun:\n",
    "                table = str.maketrans('','',string.punctuation)\n",
    "                stripped = [w.translate(table) for w in tokens]\n",
    "                if len(stripped) > 0:\n",
    "                    text_dic[_id] = stripped\n",
    "                    tokens = text_dic[_id]\n",
    "                    logger.info(f'text: {_id} | After punctuation removal: {len(tokens)}')\n",
    "\n",
    "            # remove tokens that are not in alphabetic\n",
    "            if remove_non_alphabetic:\n",
    "                alpha_words = [word for word in tokens if word.isalpha()]\n",
    "                if len(alpha_words) > 0:\n",
    "                    text_dic[_id] = alpha_words\n",
    "                    tokens = text_dic[_id]\n",
    "                    logger.info(f'text: {_id} | After non alphabetic removal: {len(tokens)}')\n",
    "            \n",
    "            # lower case\n",
    "            if lower_case:\n",
    "                lower_words = [word.lower() for word in tokens]\n",
    "                text_dic[_id] = lower_words\n",
    "                tokens = text_dic[_id]\n",
    "                logger.info(f'text: {_id} | After lower case: {len(tokens)}')\n",
    "\n",
    "            \n",
    "            # filter out stop words\n",
    "            if remove_stop_words:\n",
    "                stop_words = set(stopwords.words('english'))\n",
    "                new_list = set(list(stop_words) + custom_stop_words)\n",
    "                not_stop_words = [w for w in tokens if not w in stop_words]\n",
    "                if len(not_stop_words) > 0:\n",
    "                    text_dic[_id] = not_stop_words\n",
    "                    tokens = text_dic[_id]\n",
    "                    logger.info(f'text: {_id} | After stop word removal: {len(tokens)}')\n",
    "            \n",
    "            # consider stemming...???\n",
    "            if stemming:\n",
    "                ps = PorterStemmer()\n",
    "                stem_words = [ps.stem(word) for word in tokens]\n",
    "                text_dic[_id] = stem_words\n",
    "                tokens = text_dic[_id]\n",
    "                logger.info(f'text: {_id} | After stemming: {len(tokens)}')\n",
    "            \n",
    "            # count tokens\n",
    "            total_tokens_after = total_tokens_after+len(text_dic[_id])\n",
    "            \n",
    "        except BaseException as be:\n",
    "            logger.warning(f'**WARNING** Caught BaseException: {be}')\n",
    "            pass\n",
    "\n",
    "    logger.info(f'Total Tokens Prior To Cleaning: {total_tokens_prior}')\n",
    "    logger.info(f'Total Tokens After Cleaning: {total_tokens_after}')\n",
    "    \n",
    "    \n",
    "    return text_dic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Split Team text into it's tokens(feature words) using [NLTK word_tokenizer](https://www.nltk.org/api/nltk.tokenize.html)**<br>\n",
    "\n",
    "**Parameters**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get raw team corpus text from file system\n",
    "_files = []\n",
    "_filenames = {}\n",
    "filenames_index = 0\n",
    "\n",
    "#path=f'{corpusDir}'\n",
    "path=f'{corpusDir}/dump_archive'\n",
    "\n",
    "for dirpath, dirs, files in os.walk(path):\n",
    "    logger.debug(f'Raw Text Corpus Directory Search: {dirpath}')\n",
    "    logger.debug(f'Raw Text Corpus Files Search: {files}')\n",
    "    logger.debug(f'Raw Text Corpus Files Search - file count: {len(files)}')\n",
    "    if len(files) > 0:\n",
    "        for f in files:\n",
    "            _files.append(f'{dirpath}/{f}')\n",
    "            _filenames[filenames_index] = f\n",
    "            filenames_index+=1\n",
    "logger.info(f'Raw Text Corpus Files Search - Files List Found:\\n{_files}')\n",
    "logger.info(f'Raw Text Corpus Files Search - Filenames Dictionary Found:\\n{_filenames}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize each teams text data\n",
    "team_tokens = {}\n",
    "teams = []\n",
    "t_tokens = []\n",
    "teams_token_totals = []\n",
    "raw_corpus_token_count = 0\n",
    "for f in _files:\n",
    "    logger.info(f'team text file: {f}')\n",
    "    team_name = f.split('/')[-1]\n",
    "    #team_name = team_name.split('.')[0]\n",
    "    logger.info(f'team name: {team_name}')\n",
    "    tokens = []\n",
    "    file_token_count = 0\n",
    "    \n",
    "    with open(f'{f}','r') as f:\n",
    "        team_text = f.readlines()\n",
    "        \n",
    "        for i,line in enumerate(team_text):\n",
    "            logger.debug(f'team text line: {i}')\n",
    "            logger.debug(f'team text line: {line}')\n",
    "            \n",
    "            tokens = word_tokenize(line)\n",
    "            file_token_count = file_token_count+len(tokens)\n",
    "            raw_corpus_token_count = raw_corpus_token_count+len(tokens)\n",
    "            \n",
    "            logger.debug(tokens)\n",
    "            \n",
    "            #break\n",
    "        team_tokens[team_name] = tokens\n",
    "        teams.append(team_name)\n",
    "        t_tokens.append(tokens)\n",
    "        teams_token_totals.append(file_token_count)\n",
    "        logger.info(f'file token count: {file_token_count}')\n",
    "        \n",
    "      \n",
    "    #break\n",
    "logger.info(f'Raw Corpus Token Count: {raw_corpus_token_count}')\n",
    "#wordcloud_draw(str(team_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = pd.DataFrame()\n",
    "tt['team'] = teams\n",
    "tt['tokens'] = t_tokens\n",
    "tt['total_tokens'] = teams_token_totals\n",
    "tt.sort_values(by=\"total_tokens\", ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt.total_tokens.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get sample of word clouds for teams\n",
    "for i,team in enumerate(team_tokens):\n",
    "    #print(team_tokens[team])\n",
    "    wordcloud_draw(team_tokens[team],color='white',max_words=250)\n",
    "    if i > 5: break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### INIT FEATURE BoW Count\n",
    "Perform initial Bag Of Words Count - save off for reference and insights into vocabular size reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_as = f'{dataDir}/init_feature_counts.csv'\n",
    "iF = count_features(team_tokens,save_as=save_as)\n",
    "bag_of_words = iF[1]\n",
    "logger.info(f'Initial Bag Of Word Feature Count: {len(bag_of_words)}')\n",
    "iF[0].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at initial word cloud of bag of words\n",
    "wordcloud_draw(bag_of_words, color='white', max_words=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Cleaning Vocabular Size Reduction* <br>\n",
    "* Total Tokens Prior To Cleaning: 18647\n",
    "* Total Tokens After Cleaning: 9879"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_text = clean_text(team_tokens,\n",
    "                     custom_stop_words=[],\n",
    "                     remove_pun=True,\n",
    "                     remove_non_alphabetic=True,\n",
    "                     remove_stop_words=True,\n",
    "                     lower_case=True,\n",
    "                     stemming=False,\n",
    "                    )\n",
    "\n",
    "kept_feats = cleaned_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KEPT FEATURES BoW Count\n",
    "Perform word frequency count for kept feature list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save text as csv\n",
    "# put the feature word counts into named dictionary and data frame for simpler sorting and observation\n",
    "kept_features = {'_id':[],'features':[]}\n",
    "for _id, features in kept_feats.items():\n",
    "    kept_features['_id'].append(_id)\n",
    "    kept_features['features'].append(features)\n",
    "\n",
    "# convert dictionary to dataframe for easier sorting\n",
    "kept_features_df = pd.DataFrame(kept_features)\n",
    "kept_features_df_sorted = kept_features_df.sort_values(by=['_id'],ascending=True)\n",
    "\n",
    "# save df as new data source\n",
    "save_as = f'{dataDir}/kept_features.csv'\n",
    "kept_features_df_sorted.to_csv(save_as,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kept_features_df_sorted.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save kept features as new corpus to be consumed by vectorization objects\n",
    "for _id, features in kept_features_df_sorted.iterrows():\n",
    "    line = ' '.join([feat for feat in kept_features_df_sorted.iloc[_id].features])\n",
    "    \n",
    "    cleanedDir = f'{corpusDir}/teams/v2/cleaned'\n",
    "    if not os.path.exists(cleanedDir): os.makedirs(cleanedDir)\n",
    "    \n",
    "    with io.open(f'{cleanedDir}/{kept_features_df_sorted.iloc[_id]._id}_nfl_team_text.txt','w+',encoding='utf8') as f:\n",
    "        f.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "save_as = f'{dataDir}/kept_lower_feature_counts.csv'\n",
    "kf = count_features(kept_feats,save_as=save_as)\n",
    "clean_bag_of_words = kf[1]\n",
    "kf[0].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean_bag_of_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at cleaned word cloud of bag of words\n",
    "wordcloud_draw(clean_bag_of_words, color='white', max_words=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get team text feature files\n",
    "#kept_feats_file = 'kept_feature_counts.csv'\n",
    "kept_lower_feats_file = 'kept_lower_feature_counts.csv'\n",
    "#kept_feats_file = 'kept_lower_stem_feature_counts.csv'\n",
    "kept_feats_file = 'kept_features.csv'\n",
    "\n",
    "kept_feats_counts = pd.read_csv(f'{dataDir}/kept_lower_feature_counts.csv',error_bad_lines=False, encoding = \"ISO-8859-1\")\n",
    "kept_feats = pd.read_csv(f'{dataDir}/{kept_feats_file}',error_bad_lines=False, encoding = \"ISO-8859-1\")\n",
    "#kept_feats_counts.head()\n",
    "kept_feats_counts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save off each team text as it's own file\n",
    "import io\n",
    "for row in kept_feats.iterrows():\n",
    "    _id = row[1]['_id']\n",
    "    features = row[1]['features']\n",
    "    features = features.replace('[','')\n",
    "    features = features.replace(']','')\n",
    "    features = features.replace('\\'','')\n",
    "    features = features.replace(',','')\n",
    "    \n",
    "    cleanedDir = f'{corpusDir}/teams/v1/cleaned'\n",
    "    if not os.path.exists(cleanedDir): os.makedirs(cleanedDir)\n",
    "    \n",
    "    with io.open(f'{cleanedDir}/{_id}_nfl_team_text.txt','w+',encoding='utf8') as f:\n",
    "        f.write(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Vectorization Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create integer feature vector mappings\n",
    "feature_id_map = {}\n",
    "id_feature_map = {}\n",
    "\n",
    "feats = kept_feats_counts.feature\n",
    "\n",
    "for i,f in enumerate(feats):\n",
    "    id_feature_map[i] = f\n",
    "    feature_id_map[f] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from time import time\n",
    "import shorttext\n",
    "#from keras.preprocessing.text import Tokenizer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "analyzer = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[sklearn CountVectorizer](https://scikit-learn.org/0.15/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer)<br>\n",
    "Convert a collection of text documents to a matrix of token counts<br>\n",
    "\n",
    "This implementation produces a sparse representation of the counts using scipy.sparse.coo_matrix.<br>\n",
    "\n",
    "If you do not provide an a-priori dictionary and you do not use an analyzer that does some kind of feature selection then the number of features will be equal to the vocabulary size found by analyzing the data<br>\n",
    "\n",
    "In text mining, it is important to create the document-term matrix (DTM) of the corpus we are interested in. A DTM is basically a matrix, with documents designated by rows and words by columns, that the elements are the counts or the weights (usually by tf-idf). Subsequent analysis is usually based creatively on DTM.<br>\n",
    "\n",
    "CountVectorizer supports counts of N-grams of words or consecutive characters. Once fitted, the vectorizer has built a dictionary of feature indices:<br>\n",
    "The index value of a word in the vocabulary is linked to its frequency in the whole training corpus.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get cleaned team text files from path\n",
    "_files = []\n",
    "_filenames = {}\n",
    "#cleanedDir = f'{corpusDir}/teams/v2/cleaned'\n",
    "path=f'{corpusDir}/teams/v2/cleaned/'\n",
    "for dirpath, dirs, files in os.walk(path):\n",
    "    print(dirpath)\n",
    "    print(dirs)\n",
    "    print(files)\n",
    "    for i,f in enumerate(files):\n",
    "        _files.append(dirpath+f)\n",
    "        _filenames[i] = f\n",
    "#_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_files[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorization Objects to Explore\n",
    "### CountVectorizer & TfidfVectorizer\n",
    "* unigrams\n",
    "* bigrams\n",
    "* trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inst_vectorizer(ngram_type,vectorizer_type,input='filename',max_df=1.0,min_df=1,stop_words='english',analyzer='word',max_features=None):\n",
    "    vectorizer = None\n",
    "    ngram = (1,1)\n",
    "    # set ngram type\n",
    "    if ngram_type == 'unigram':\n",
    "        ngram = (1,1)\n",
    "    elif ngram_type == 'bigram':\n",
    "        ngram = (1,2)\n",
    "    elif ngram_type == 'trigram':\n",
    "        ngram = (1,3)\n",
    "    else:\n",
    "        ngram = (1,1)\n",
    "        \n",
    "    if vectorizer_type == 'count':\n",
    "        vectorizer = CountVectorizer(input=input,ngram_range=ngram,max_df=max_df,min_df=min_df,stop_words=stop_words,analyzer=analyzer,max_features=max_features)\n",
    "    elif vectorizer_type == 'tfidf':\n",
    "        vectorizer = TfidfVectorizer(input=input,ngram_range=ngram,max_df=max_df,min_df=min_df,stop_words=stop_words,analyzer=analyzer,max_features=max_features)\n",
    "    else:\n",
    "        vectorizer = CountVectorizer(input=input,ngram_range=ngram,max_df=max_df,min_df=min_df,stop_words=stop_words,analyzer=analyzer,max_features=max_features)\n",
    "\n",
    "    return vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize a CountVectorizer object:\n",
    "# Initialize a unigram vector object\n",
    "count_vec_unigram = inst_vectorizer('unigram','count')\n",
    "tfidf_vec_unigram = inst_vectorizer('unigram','tfidf')\n",
    "#-------------------------------------------------------#\n",
    "# Initialize a bigram vector object\n",
    "count_vec_bigram = inst_vectorizer('bigram','count')\n",
    "tfidf_vec_bigram = inst_vectorizer('bigram','tfidf')\n",
    "#-------------------------------------------------------#\n",
    "# Initialize a trigram vector object\n",
    "count_vec_trigram = inst_vectorizer('trigram','count')\n",
    "tfidf_vec_trigram = inst_vectorizer('trigram','tfidf')\n",
    "#-------------------------------------------------------#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CountVectorize Unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the data into a bag of words\n",
    "count_unigram = count_vec_unigram.fit(_files)\n",
    "count_bow_unigram = count_vec_unigram.transform(_files)\n",
    "\n",
    "count_unigram_features = count_unigram.get_feature_names()\n",
    "\n",
    "# print a few of the features\n",
    "logger.info(f'CountVectorizer unigram transformed shape: {count_bow_unigram.shape}')\n",
    "logger.info(f'CountVectorizer unigram transformed size: {count_bow_unigram.size}')\n",
    "logger.info(f'CountVectorizer unigram transformed type: {type(count_bow_unigram)}')\n",
    "#logger.info(f'List of all ngram features:\\n{count_unigram_features}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(count_unigram_features)\n",
    "count_unigram_features[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(f'unigram vocabulary size: {len(count_vec_unigram.vocabulary_)}')\n",
    "#logger.info(f'ngram vocabulary content:\\n {count_vec_unigram.vocabulary_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt_uni_voc_dict = dict(count_unigram.vocabulary_)\n",
    "cnt_uni_voc_df = pd.DataFrame.from_dict(cnt_uni_voc_dict, orient='index').reset_index()\n",
    "cnt_uni_voc_df.columns=('feature','feature_index')\n",
    "cnt_uni_voc_df.sort_values(by='feature_index', ascending=False)[::10].head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output CountVectorize unigram feature vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_id_map = {}\n",
    "id_word_map = {}\n",
    "\n",
    "words = count_unigram.get_feature_names()\n",
    "logger.debug(words)\n",
    "\n",
    "for i,f in enumerate(words):\n",
    "    word_id_map[i] = f\n",
    "    id_word_map[f] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = count_unigram.get_feature_names()\n",
    "tdm_vec_df = pd.DataFrame(count_bow_unigram.toarray(),columns=cols)\n",
    "non_zero_field_count = 0\n",
    "# output feature vector term frequence vector as 'doc feature frequency'\n",
    "with open(f'{outputDir}/count_unigram_feature_vector_tf.txt','w+') as f:\n",
    "\n",
    "    for i in range(0,tdm_vec_df.shape[0]):\n",
    "        a = [index for index,value in enumerate(tdm_vec_df.iloc[i]) if value > 0]\n",
    "        sent = _filenames[i]\n",
    "        non_zero_field_count = non_zero_field_count+len(a)\n",
    "        \n",
    "        for col in a:\n",
    "            v = tdm_vec_df.iloc[i,col]\n",
    "            sent = sent+' '+word_id_map[col]+' '+str(v)\n",
    "        \n",
    "        f.write(sent+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdm_vec_df[::5].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TfidfVectorize Unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the data into a bag of words\n",
    "tfidf_unigram = tfidf_vec_unigram.fit(_files)\n",
    "tfidf_bow_unigram = tfidf_vec_unigram.transform(_files)\n",
    "\n",
    "tfidf_unigram_features = tfidf_unigram.get_feature_names()\n",
    "\n",
    "# print a few of the features\n",
    "logger.info(f'TfidfVectorizer unigram transformed shape: {tfidf_bow_unigram.shape}')\n",
    "logger.info(f'TfidfVectorizer unigram transformed size: {tfidf_bow_unigram.size}')\n",
    "logger.info(f'TfidfVectorizer unigram transformed type: {type(tfidf_bow_unigram)}')\n",
    "#logger.info(f'List of all ngram features:\\n{count_unigram_features}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tfidf_unigram_features)\n",
    "tfidf_unigram_features[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(f'tfidf_unigram vocabulary size: {len(tfidf_unigram.vocabulary_)}')\n",
    "#logger.info(f'ngram vocabulary content:\\n {tfidf_unigram.vocabulary_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get and eval the IDF: The inverse document frequency\n",
    "idf_unigram = tfidf_unigram.idf_\n",
    "idf_weights = dict(zip(tfidf_unigram.get_feature_names(), idf_unigram))\n",
    "\n",
    "idf_weights_df = pd.DataFrame.from_dict(idf_weights,orient='index').reset_index()\n",
    "idf_weights_df.columns=('feature','weight')\n",
    "idf_weights_df = idf_weights_df.sort_values(by='weight',ascending=False)\n",
    "\n",
    "logger.info(f'IDF Top 10 List:\\n{idf_weights_df.head(10)}')\n",
    "logger.info(f'IDF Lowest 10 List:\\n{idf_weights_df.tail(10).sort_values(by=\"weight\",ascending=True)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "sns.barplot(x='feature', y='weight', data=idf_weights_df)            \n",
    "plt.title(\"Unigram Inverse Document Frequency(idf) per token\")\n",
    "fig=plt.gcf()\n",
    "fig.set_size_inches(10,5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_id_map = {}\n",
    "id_word_map = {}\n",
    "\n",
    "words = tfidf_unigram.get_feature_names()\n",
    "logger.debug(words)\n",
    "\n",
    "for i,f in enumerate(words):\n",
    "    word_id_map[i] = f\n",
    "    id_word_map[f] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = tfidf_unigram.get_feature_names()\n",
    "tdm_vec_df = pd.DataFrame(tfidf_bow_unigram.toarray(),columns=cols)\n",
    "non_zero_field_count = 0\n",
    "# output feature vector term frequence vector as 'doc feature frequency'\n",
    "with open(f'{outputDir}/tfidf_unigram_feature_vector_tf.txt','w+') as f:\n",
    "\n",
    "    for i in range(0,tdm_vec_df.shape[0]):\n",
    "        a = [index for index,value in enumerate(tdm_vec_df.iloc[i]) if value > 0]\n",
    "        sent = _filenames[i]\n",
    "        non_zero_field_count = non_zero_field_count+len(a)\n",
    "        \n",
    "        for col in a:\n",
    "            v = tdm_vec_df.iloc[i,col]\n",
    "            sent = sent+' '+word_id_map[col]+' '+str(v)\n",
    "        \n",
    "        f.write(sent+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_tfidf = tfidf_bow_unigram.max(axis=0).toarray().ravel()\n",
    "sort_by_tfidf = max_tfidf.argsort()\n",
    "logger.info(f'Features weith lowest tfidf:\\n{sort_by_tfidf[:5]}')\n",
    "logger.info(f'Features weith heights tfidf:\\n{sort_by_tfidf[-5:]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_uni_voc_dict = dict(tfidf_unigram.vocabulary_)\n",
    "tfidf_uni_voc_df = pd.DataFrame.from_dict(tfidf_uni_voc_dict, orient='index').reset_index()\n",
    "tfidf_uni_voc_df.columns=('feature','feature_index')\n",
    "tfidf_uni_voc_df.sort_values(by='feature_index', ascending=False)[::10].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features with the lowest 5 tfidf\n",
    "low_5_tfidf = sort_by_tfidf[:5]\n",
    "low_tfidf = tfidf_uni_voc_df[tfidf_uni_voc_df.feature_index.isin(low_5_tfidf)]\n",
    "low_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features with the highest 5 tfidf\n",
    "high_5_tfidf = sort_by_tfidf[-5:]\n",
    "high_tfidf = tfidf_uni_voc_df[tfidf_uni_voc_df.feature_index.isin(high_5_tfidf)]\n",
    "high_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CountVectorize - Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eval Bigrams\n",
    "count_bigram = count_vec_bigram.fit(_files)\n",
    "cnt_bow_bigram = count_vec_bigram.transform(_files)\n",
    "\n",
    "cnt_bigrams = count_vec_bigram.get_feature_names()\n",
    "\n",
    "# print a few of the features\n",
    "logger.info(f'CountVectorizer ngram transformed shape: {cnt_bow_bigram.shape}')\n",
    "logger.info(f'CountVectorizer ngram transformed size: {cnt_bow_bigram.size}')\n",
    "logger.info(f'CountVectorizer ngram transformed type: {type(cnt_bow_bigram)}')\n",
    "#logger.info(f'List of all bigram features:\\n{cnt_bigrams}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(f'bigram vocabulary size: {len(count_bigram.vocabulary_)}')\n",
    "#logger.info(f'bigram vocabulary content:\\n {count_vec_bigram.vocabulary_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#logger.info(f'CountVectorizer Fit: \\n{fit_vec.vocabulary_}')\n",
    "voc_dict = dict(count_bigram.vocabulary_)\n",
    "voc_df = pd.DataFrame.from_dict(voc_dict, orient='index').reset_index()\n",
    "voc_df.columns=('feature','feature_index')\n",
    "voc_df.sort_values(by='feature_index', ascending=False)[::10].head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output CountVectorize bigram feature vector to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_id_map = {}\n",
    "id_word_map = {}\n",
    "\n",
    "words = count_bigram.get_feature_names()\n",
    "logger.debug(words)\n",
    "\n",
    "for i,f in enumerate(words):\n",
    "    word_id_map[i] = f\n",
    "    id_word_map[f] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = count_bigram.get_feature_names()\n",
    "tdm_vec_df = pd.DataFrame(cnt_bow_bigram.toarray(),columns=cols)\n",
    "non_zero_field_count = 0\n",
    "# output feature vector term frequence vector as 'doc feature frequency'\n",
    "with open(f'{outputDir}/cnt_bigrams_feature_vector_tf.txt','w+') as f:\n",
    "\n",
    "    for i in range(0,tdm_vec_df.shape[0]):\n",
    "        a = [index for index,value in enumerate(tdm_vec_df.iloc[i]) if value > 0]\n",
    "        sent = _filenames[i]\n",
    "        non_zero_field_count = non_zero_field_count+len(a)\n",
    "        \n",
    "        for col in a:\n",
    "            v = tdm_vec_df.iloc[i,col]\n",
    "            sent = sent+' '+word_id_map[col]+' '+str(v)\n",
    "        \n",
    "        f.write(sent+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tdm_vec_df[::5].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TfidfVectorize Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the data into a bag of words\n",
    "tfidf_bigram = tfidf_vec_bigram.fit(_files)\n",
    "tfidf_bow_bigram = tfidf_vec_bigram.transform(_files)\n",
    "\n",
    "tfidf_bigram_features = tfidf_bigram.get_feature_names()\n",
    "\n",
    "# print a few of the features\n",
    "logger.info(f'TfidfVectorizer bigram transformed shape: {tfidf_bow_bigram.shape}')\n",
    "logger.info(f'TfidfVectorizer bigram transformed size: {tfidf_bow_bigram.size}')\n",
    "logger.info(f'TfidfVectorizer bigram transformed type: {type(tfidf_bow_bigram)}')\n",
    "#logger.info(f'List of all bigram features:\\n{count_unigram_features}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tfidf_bigram_features)\n",
    "tfidf_bigram_features[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get and eval the IDF: The inverse document frequency\n",
    "idf_bigram = tfidf_bigram.idf_\n",
    "idf_weights = dict(zip(tfidf_bigram.get_feature_names(), idf_bigram))\n",
    "\n",
    "idf_weights_df = pd.DataFrame.from_dict(idf_weights,orient='index').reset_index()\n",
    "idf_weights_df.columns=('feature','weight')\n",
    "idf_weights_df = idf_weights_df.sort_values(by='weight',ascending=False)\n",
    "\n",
    "logger.info(f'IDF bigram Top 10 List:\\n{idf_weights_df.head(10)}')\n",
    "logger.info(f'IDF bigram Lowest 10 List:\\n{idf_weights_df.tail(10).sort_values(by=\"weight\",ascending=True)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "#sns.barplot(x='feature', y='weight', data=idf_weights_df)            \n",
    "#plt.title(\"Bigram Inverse Document Frequency(idf) per token\")\n",
    "#fig=plt.gcf()\n",
    "#fig.set_size_inches(10,5)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_id_map = {}\n",
    "id_word_map = {}\n",
    "\n",
    "words = tfidf_bigram.get_feature_names()\n",
    "logger.debug(words)\n",
    "\n",
    "for i,f in enumerate(words):\n",
    "    word_id_map[i] = f\n",
    "    id_word_map[f] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = tfidf_bigram.get_feature_names()\n",
    "tdm_vec_df = pd.DataFrame(tfidf_bow_bigram.toarray(),columns=cols)\n",
    "non_zero_field_count = 0\n",
    "# output feature vector term frequence vector as 'doc feature frequency'\n",
    "with open(f'{outputDir}/tfidf_bigram_feature_vector_tf.txt','w+') as f:\n",
    "\n",
    "    for i in range(0,tdm_vec_df.shape[0]):\n",
    "        a = [index for index,value in enumerate(tdm_vec_df.iloc[i]) if value > 0]\n",
    "        sent = _filenames[i]\n",
    "        non_zero_field_count = non_zero_field_count+len(a)\n",
    "        \n",
    "        for col in a:\n",
    "            v = tdm_vec_df.iloc[i,col]\n",
    "            sent = sent+' '+word_id_map[col]+' '+str(v)\n",
    "        \n",
    "        f.write(sent+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_bi_voc_dict = dict(tfidf_bigram.vocabulary_)\n",
    "tfidf_bi_voc_df = pd.DataFrame.from_dict(tfidf_bi_voc_dict, orient='index').reset_index()\n",
    "tfidf_bi_voc_df.columns=('feature','feature_index')\n",
    "tfidf_bi_voc_df.sort_values(by='feature_index', ascending=False)[::10].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_tfidf = tfidf_bow_bigram.max(axis=0).toarray().ravel()\n",
    "sort_by_tfidf = max_tfidf.argsort()\n",
    "logger.info(f'Features weith lowest tfidf:\\n{sort_by_tfidf[:5]}')\n",
    "logger.info(f'Features weith heights tfidf:\\n{sort_by_tfidf[-5:]}')\n",
    "# features with the lowest 5 tfidf\n",
    "low_5_tfidf = sort_by_tfidf[:5]\n",
    "low_tfidf = tfidf_bi_voc_df[tfidf_bi_voc_df.feature_index.isin(low_5_tfidf)]\n",
    "low_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features with the highest 5 tfidf\n",
    "high_5_tfidf = sort_by_tfidf[-5:]\n",
    "high_tfidf = tfidf_bi_voc_df[tfidf_bi_voc_df.feature_index.isin(high_5_tfidf)]\n",
    "high_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorizer - Trigram Eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CountVectorize Trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eval Trigram\n",
    "count_trigram = count_vec_trigram.fit(_files)\n",
    "cnt_bow_trigram = count_vec_trigram.transform(_files)\n",
    "\n",
    "cnt_trigrams = count_vec_trigram.get_feature_names()\n",
    "\n",
    "# print a few of the features\n",
    "logger.info(f'CountVectorizer trigram transformed shape: {cnt_bow_trigram.shape}')\n",
    "logger.info(f'CountVectorizer trigram transformed size: {cnt_bow_trigram.size}')\n",
    "logger.info(f'CountVectorizer trigram transformed type: {type(cnt_trigrams)}')\n",
    "#logger.info(f'List of all trigrams features:\\n{trigrams}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(f'trigram vocabulary size: {len(count_vec_trigram.vocabulary_)}')\n",
    "#logger.info(f'trigram vocabulary content:\\n {count_vec_trigram.vocabulary_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#logger.info(f'CountVectorizer Fit: \\n{fit_vec.vocabulary_}')\n",
    "voc_dict = dict(count_trigram.vocabulary_)\n",
    "voc_df = pd.DataFrame.from_dict(voc_dict, orient='index').reset_index()\n",
    "voc_df.columns=('feature','feature_index')\n",
    "voc_df.sort_values(by='feature_index', ascending=False)[::10].head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output CountVectorize trigram feature vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_id_map = {}\n",
    "id_word_map = {}\n",
    "\n",
    "words = count_trigram.get_feature_names()\n",
    "logger.debug(words)\n",
    "\n",
    "for i,f in enumerate(words):\n",
    "    word_id_map[i] = f\n",
    "    id_word_map[f] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = count_trigram.get_feature_names()\n",
    "tdm_vec_df = pd.DataFrame(cnt_bow_trigram.toarray(),columns=cols)\n",
    "non_zero_field_count = 0\n",
    "# output feature vector term frequence vector as 'doc feature frequency'\n",
    "with open(f'{outputDir}/cnt_trigrams_feature_vector_tf.txt','w+') as f:\n",
    "\n",
    "    for i in range(0,tdm_vec_df.shape[0]):\n",
    "        a = [index for index,value in enumerate(tdm_vec_df.iloc[i]) if value > 0]\n",
    "        sent = _filenames[i]\n",
    "        non_zero_field_count = non_zero_field_count+len(a)\n",
    "        \n",
    "        for col in a:\n",
    "            v = tdm_vec_df.iloc[i,col]\n",
    "            sent = sent+' '+word_id_map[col]+' '+str(v)\n",
    "        \n",
    "        f.write(sent+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdm_vec_df[::5].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TfidfVectorize Trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_words(words):\n",
    "    l = words.split(' ')\n",
    "    combined = ''\n",
    "    if len(l)>1:\n",
    "        for i,w in enumerate(l):\n",
    "            if i==0:\n",
    "                combined = w\n",
    "            else:\n",
    "                combined = combined+'_'+w\n",
    "    else:\n",
    "        combined = words\n",
    "    return combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classify trigram for comparative analysis\n",
    "def sentiment_classify(sentence):\n",
    "    vs = analyzer.polarity_scores(sentence)\n",
    "    #pol_scores.append(vs)\n",
    "    label = classify_vader_score_threshold(vs['compound'])\n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the data into a bag of words\n",
    "tfidf_trigram = tfidf_vec_trigram.fit(_files)\n",
    "tfidf_bow_trigram = tfidf_vec_trigram.transform(_files)\n",
    "\n",
    "tfidf_trigram_features = tfidf_trigram.get_feature_names()\n",
    "\n",
    "# print a few of the features\n",
    "logger.info(f'TfidfVectorizer trigram transformed shape: {tfidf_bow_trigram.shape}')\n",
    "logger.info(f'TfidfVectorizer trigram transformed size: {tfidf_bow_trigram.size}')\n",
    "logger.info(f'TfidfVectorizer trigram transformed type: {type(tfidf_bow_trigram)}')\n",
    "#logger.info(f'List of all bigram features:\\n{count_unigram_features}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tfidf_trigram_features)\n",
    "tfidf_trigram_features[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get and eval the IDF: The inverse document frequency\n",
    "idf_trigram = tfidf_trigram.idf_\n",
    "idf_weights = dict(zip(tfidf_trigram.get_feature_names(), idf_trigram))\n",
    "\n",
    "idf_weights_df = pd.DataFrame.from_dict(idf_weights,orient='index').reset_index()\n",
    "idf_weights_df.columns=('feature','weight')\n",
    "idf_weights_df = idf_weights_df.sort_values(by='weight',ascending=False)\n",
    "\n",
    "logger.info(f'IDF trigram Top 10 List:\\n{idf_weights_df.head(10)}')\n",
    "logger.info(f'IDF trigram Lowest 10 List:\\n{idf_weights_df.tail(10).sort_values(by=\"weight\",ascending=True)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "sns.barplot(x='feature', y='weight', data=idf_weights_df)            \n",
    "plt.title(\"Trigram Inverse Document Frequency(idf) per token\")\n",
    "fig=plt.gcf()\n",
    "fig.set_size_inches(10,5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_id_map = {}\n",
    "id_word_map = {}\n",
    "\n",
    "words = tfidf_trigram.get_feature_names()\n",
    "logger.debug(words)\n",
    "\n",
    "for i,f in enumerate(words):\n",
    "    word_id_map[i] = f\n",
    "    id_word_map[f] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = tfidf_trigram.get_feature_names()\n",
    "tdm_vec_df = pd.DataFrame(tfidf_bow_trigram.toarray(),columns=cols)\n",
    "non_zero_field_count = 0\n",
    "# output feature vector term frequence vector as 'doc feature frequency'\n",
    "with open(f'{outputDir}/tfidf_trigram_feature_vector_tf.txt','w+') as f:\n",
    "\n",
    "    for i in range(0,tdm_vec_df.shape[0]):\n",
    "        a = [index for index,value in enumerate(tdm_vec_df.iloc[i]) if value > 0]\n",
    "        sent = _filenames[i]\n",
    "        non_zero_field_count = non_zero_field_count+len(a)\n",
    "        \n",
    "        for col in a:\n",
    "            v = tdm_vec_df.iloc[i,col]\n",
    "            sent = sent+' '+word_id_map[col]+' '+str(v)\n",
    "        \n",
    "        f.write(sent+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labled with sentiment\n",
    "cols = tfidf_trigram.get_feature_names()\n",
    "tdm_vec_df = pd.DataFrame(tfidf_bow_trigram.toarray(),columns=cols)\n",
    "non_zero_field_count = 0\n",
    "# output feature vector term frequence vector as 'doc feature frequency'\n",
    "with open(f'{outputDir}/tfidf_labeled_trigram_feature_vector_tf.txt','w+') as f:\n",
    "\n",
    "    for i in range(0,tdm_vec_df.shape[0]):\n",
    "        a = [index for index,value in enumerate(tdm_vec_df.iloc[i]) if value > 0]\n",
    "        sent = _filenames[i]\n",
    "        non_zero_field_count = non_zero_field_count+len(a)\n",
    "        \n",
    "        for col in a:\n",
    "            v = tdm_vec_df.iloc[i,col]\n",
    "            sent = sent+' '+join_words(word_id_map[col])+'|'+str(v)+'|'+sentiment_classify(word_id_map[col])\n",
    "        \n",
    "        f.write(sent+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_tfidf = tfidf_bow_trigram.max(axis=0).toarray().ravel()\n",
    "sort_by_tfidf = max_tfidf.argsort()\n",
    "logger.info(f'Features weith lowest tfidf:\\n{sort_by_tfidf[:5]}')\n",
    "logger.info(f'Features weith heights tfidf:\\n{sort_by_tfidf[-5:]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_tri_voc_dict = dict(tfidf_trigram.vocabulary_)\n",
    "tfidf_tri_voc_df = pd.DataFrame.from_dict(tfidf_tri_voc_dict, orient='index').reset_index()\n",
    "tfidf_tri_voc_df.columns=('feature','feature_index')\n",
    "tfidf_tri_voc_df.sort_values(by='feature_index', ascending=False)[::10].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features with the lowest 5 tfidf\n",
    "low_5_tfidf = sort_by_tfidf[:5]\n",
    "low_tfidf = tfidf_tri_voc_df[tfidf_tri_voc_df.feature_index.isin(low_5_tfidf)]\n",
    "low_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features with the highest 5 tfidf\n",
    "high_5_tfidf = sort_by_tfidf[-5:]\n",
    "high_tfidf = tfidf_tri_voc_df[tfidf_tri_voc_df.feature_index.isin(high_5_tfidf)]\n",
    "high_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[sklearn TfidfVectorizer](https://scikit-learn.org/0.15/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer)<br>\n",
    "\n",
    "To avoid these potential discrepancies it suffices to divide the number of occurrences of each word in a document by the total number of words in the document: these new features are called tf for Term Frequencies.<br>\n",
    "\n",
    "Another refinement on top of tf is to downscale weights for words that occur in many documents in the corpus and are therefore less informative than those that occur only in a smaller portion of the corpus.<br>\n",
    "\n",
    "This downscaling is called tf–idf for “Term Frequency times Inverse Document Frequency”.<br>\n",
    "\n",
    "Both tf and tf–idf can be computed as follows using TfidfTransformer:<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Weight of tokens per document**<br>\n",
    "the more times a token appears in a document, the more weight it will have. However, the more documents the token appears in, it is 'penalized' and the weight is diminished. For example, the weight for token 'not' is 4, but if it did not appear in all documents (that is, only in one document) its weight would have been 8.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TF-IDF - Maximum token value throughout the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_vader_score_threshold(compound_score):\n",
    "    pos_sent = 'positive'\n",
    "    neu_sent = 'neutral'\n",
    "    neg_sent = 'negative'\n",
    "    sentiment_class = ''\n",
    "    \n",
    "    if compound_score >= 0.05:\n",
    "        sentiment_class = pos_sent;\n",
    "    elif compound_score > -0.05 and compound_score < 0.05:\n",
    "        sentiment_class = neu_sent;\n",
    "    elif compound_score <= -0.05:\n",
    "        sentiment_class = neg_sent;\n",
    "    else:\n",
    "        logger.warning(f'classify_vader_score_threshold: compound score not in range: {compound_score}')\n",
    "    return sentiment_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "pol_scores=[]\n",
    "trigram_sentences = []\n",
    "with open(f'{outputDir}/vader_trigram_sentiment.txt','w+') as f:\n",
    "    for sentence in tfidf_trigram.vocabulary_:\n",
    "        trigram_sentences.append(sentence)\n",
    "        vs = analyzer.polarity_scores(sentence)\n",
    "        pol_scores.append(vs)\n",
    "        label = classify_vader_score_threshold(vs['compound'])\n",
    "        f.write(\"{0:-<65} {1} {2}\".format(sentence, str(vs), label)+'\\n')\n",
    "        print(\"{0:-<65} {1} {2}\".format(sentence, str(vs), label)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the polarity scoring\n",
    "pol_scores_df = pd.DataFrame(pol_scores)\n",
    "logger.info(pol_scores_df.describe())\n",
    "#pol_scores_df.head()\n",
    "\n",
    "# classify each sentence as 'positive', 'negative' or 'neutral' - see function above\n",
    "sentiment_classes = [classify_vader_score_threshold(c) for c in pol_scores_df['compound'] ]\n",
    "pol_scores_df['sentiment_label'] = sentiment_classes\n",
    "pol_scores_df['ngram'] = trigram_sentences\n",
    "pol_scores_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the polarity scoring\n",
    "pol_scores_df = pd.DataFrame(pol_scores)\n",
    "logger.info(pol_scores_df.describe())\n",
    "#pol_scores_df.head()\n",
    "\n",
    "# classify each sentence as 'positive', 'negative' or 'neutral' - see function above\n",
    "sentiment_classes = [classify_vader_score_threshold(c) for c in pol_scores_df['compound'] ]\n",
    "pol_scores_df['sentiment_label'] = sentiment_classes\n",
    "pol_scores_df.head()\n",
    "sns.scatterplot(x='neg',y='pos', hue='sentiment_label', data=pol_scores_df);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(f'Negative Count: {len(pol_scores_df[pol_scores_df[\"sentiment_label\"] == \"negative\"])}')\n",
    "logger.info(f'Positive Count: {len(pol_scores_df[pol_scores_df[\"sentiment_label\"] == \"positive\"])}')\n",
    "logger.info(f'Neutral Count: {len(pol_scores_df[pol_scores_df[\"sentiment_label\"] == \"neutral\"])}')\n",
    "\n",
    "neg_count = len(pol_scores_df[pol_scores_df[\"sentiment_label\"] == \"negative\"])\n",
    "pos_count = len(pol_scores_df[pol_scores_df[\"sentiment_label\"] == \"positive\"])\n",
    "\n",
    "print(neg_count/pos_count)\n",
    "print(pos_count/neg_count)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='sentiment_label', data=pol_scores_df);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in trigram classified per team document\n",
    "#teams = {}\n",
    "tri_labeled_df = pd.DataFrame()\n",
    "\n",
    "with open(f'{outputDir}/tfidf_labeled_trigram_feature_vector_tf.txt','r') as f:\n",
    "    lines = f.readlines()\n",
    "    team_names = []\n",
    "    terms = []\n",
    "    tfidf_weights = []\n",
    "    term_labels = []\n",
    "    for team_line in lines:\n",
    "        \n",
    "        #logger.info(tokens[:5])\n",
    "        tokens = team_line.split(' ')\n",
    "        teams = tokens[0].split('_')\n",
    "        team = teams[0]+'_'+teams[1]\n",
    "        \n",
    "        for token in tokens[1:]:\n",
    "            team_names.append(team)\n",
    "            tok = token.split('|')\n",
    "            terms.append(' '.join(tok[0].split('_')))\n",
    "            tfidf_weights.append(tok[1])\n",
    "            term_labels.append(tok[2].replace('\\n',''))\n",
    "            \n",
    "        #logger.info(len(terms))\n",
    "        #logger.info(len(tfidf_weights))\n",
    "        #logger.info(len(term_labels))\n",
    "        #logger.info(len(team_names))\n",
    "            #break\n",
    "        #teams[team] = {'terms':terms, 'tfidf_weights':tfidf_weights, 'term_label':term_labels}\n",
    "        logger.info(team)\n",
    "        #break\n",
    "    tri_labeled_df['team'] = team_names\n",
    "    tri_labeled_df['term'] = terms\n",
    "    tri_labeled_df['tfidf_weight'] = tfidf_weights\n",
    "    tri_labeled_df['term_label'] = term_labels\n",
    "\n",
    "tri_labeled_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_label(l):\n",
    "    if l == 'positive':\n",
    "        return 1\n",
    "    elif l == 'negative':\n",
    "        return -1\n",
    "    elif l == 'neutral':\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = tri_labeled_df['term_label']\n",
    "label_scores = [score_label(l) for l in labels]\n",
    "tri_labeled_df['label_scores'] = label_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tri_labeled_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teams_sentiment_rank = {}\n",
    "team_score = tri_labeled_df.groupby(['team']).sum()['label_scores'].reset_index()\n",
    "#team_score = \n",
    "#team_score.columns('team','label_score_sum')\n",
    "team_score.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#team_score.plot.bar(x='team',y='label_scores',orient='h')\n",
    "\n",
    "# Initialize the matplotlib figure\n",
    "sns.set(style=\"whitegrid\")\n",
    "f, ax = plt.subplots(figsize=(6, 15))\n",
    "\n",
    "# Plot the total crashes\n",
    "sns.set_color_codes(\"pastel\")\n",
    "sns.barplot(x=\"label_scores\", y=\"team\", data=team_score.sort_values(by='label_scores', ascending=False),\n",
    "            label=\"Total\", color=\"b\")\n",
    "\n",
    "# Add a legend and informative axis label\n",
    "ax.legend(ncol=2, loc=\"lower right\", frameon=True)\n",
    "ax.set(ylabel=\"NFL Teams\",\n",
    "       xlabel=\"Sentiment Score Total\",\n",
    "      title=\"NFL Team Media Sentiment Classification Score Ranking\")\n",
    "sns.despine(left=True, bottom=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
