{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Mine Twitter - NFL Search Patterns\n",
    "\n",
    "\n",
    "Ryan Timbrook (RTIMBROO)  \n",
    "DATE:11/30/2019 <br>\n",
    "Topic: Search Twitter for tweets on specific NFL Players, Coaches, and Teams\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Objective\n",
    "_____________________________________________________________________________________________\n",
    "Capture popular opinion of peoples tweets on certain NFL characters. \n",
    "Create a corpus of tweets for sentiment analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______________________________________________________________________________________________\n",
    "### Coding Environment Setup\n",
    "Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages for analysis and modeling\n",
    "import pandas as pd #data frame operations\n",
    "import numpy as np #arrays and math functions\n",
    "import requests\n",
    "import os\n",
    "import io\n",
    "import pickle\n",
    "import re\n",
    "import sys\n",
    "import string\n",
    "from os import path\n",
    "from datetime import date\n",
    "from datetime import time\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# packages for twitter\n",
    "import tweepy as tw\n",
    "from tweepy import OAuthHandler\n",
    "import json\n",
    "from tweepy import Stream\n",
    "from tweepy.streaming import StreamListener\n",
    "\n",
    "# Twython packages for twitter\n",
    "from twython import Twython\n",
    "\n",
    "# packages for NLTK\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import TweetTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custome python packages\n",
    "import rtimbroo_utils as br             # custome python helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set global properties\n",
    "notebook_file_name = 'search_twitter_nfl_player'\n",
    "report_file_name = 'search_twitter_nfl_player'\n",
    "app_name = 'search_twitter_nfl_player'\n",
    "log_level = 10 # 10-DEBUG, 20-INFO, 30-WARNING, 40-ERROR, 50-CRITICAL\n",
    "\n",
    "# setup working directory structure\n",
    "# set global properties\n",
    "dataDir = './data'\n",
    "outputDir = './output'\n",
    "configDir = './config'\n",
    "logOutDir = './logs'\n",
    "imageDir = './images'\n",
    "modelDir = './models'\n",
    "corpusDir = './corpus'\n",
    "# create base output directories if they don't exist\n",
    "if not os.path.exists(outputDir): os.mkdir(outputDir)\n",
    "if not os.path.exists(logOutDir): os.mkdir(logOutDir)\n",
    "if not os.path.exists(imageDir): os.mkdir(imageDir)\n",
    "if not os.path.exists(modelDir): os.mkdir(modelDir)\n",
    "if not os.path.exists(dataDir): os.mkdir(dataDir)\n",
    "if not os.path.exists(configDir): os.mkdir(configDir)\n",
    "if not os.path.exists(corpusDir): os.mkdir(corpusDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a logger for troubleshooting / data exploration\n",
    "logger = br.getFileLogger(logOutDir+'/',app_name,level=log_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get current date\n",
    "now = datetime.utcnow().isoformat()\n",
    "collection_date = re.findall('^[0-9]{4}-[0-9]{2}-[0-9]{2}',now)\n",
    "collection_date = collection_date[0]\n",
    "collection_date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. OBTAIN the data   \n",
    "________________________________________________________________________________________________\n",
    "Import external datasets for evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Twitter Search API Limits:\n",
    "[Sandbox Package](https://developer.twitter.com/en/pricing/search-fullarchive)\n",
    "\n",
    "* Time frame:\tFull history\n",
    "* Tweets per request:\t100\n",
    "* Counts vs. data:\tData only\n",
    "* Query length:\t128 characters\n",
    "* Operator availability:\tStandard\n",
    "* Rate limit per minute:\t30 requests/min\n",
    "* Enrichments:\tn/a\n",
    "* Dev environments:\t1\t\n",
    "* Monthly Tweet cap:\t5k\t\n",
    "* Rate limit per second: 10 requests/sec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate Twitter API Object\n",
    "Using Twython 3.6 Twitter API Wrapper\n",
    "[Twython 3.6.0 reference documentation](https://twython.readthedocs.io/en/latest/api.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load twitter credentials\n",
    "with open(f'{configDir}/twitter_credentials.json', 'r') as f:\n",
    "    tw_cred = json.load(f)\n",
    "\n",
    "# setup client header arguments to pass along to the API\n",
    "client_args = {\n",
    "    'headers':{\n",
    "        'User-Agent': 'AI_Public_Sentiment_16860838'\n",
    "    },\n",
    "    'timeout':300,\n",
    "    \n",
    "}\n",
    "    \n",
    "# instantiate object\n",
    "py_tweets = Twython(tw_cred['CONSUMER_KEY'],\n",
    "                    tw_cred['CONSUMER_SECRET'],\n",
    "                    tw_cred['ACCESS_TOKEN'],\n",
    "                    tw_cred['ACCESS_SECRET'],\n",
    "                   client_args=client_args)\n",
    "\n",
    "logger.debug(f'{py_tweets.verify_credentials()}')\n",
    "logger.debug(f'{py_tweets.get_home_timeline()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Search Terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set what to search on\n",
    "nfl_type = 'player'\n",
    "search_on = 'deshaun_watson'\n",
    "\n",
    "# setup base twitter search query\n",
    "search_terms=\"deshaun watson\"\n",
    "\n",
    "# add filters to search criteria\n",
    "filtered_search_terms = search_terms + \" -filter:retweets\"\n",
    "\n",
    "#search_start_date = '2019-11-23' # limits to the last 7 days\n",
    "\n",
    "# number of tweets to return\n",
    "num_tweets = 100 # sandbox rate limit - 100 tweets per request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search dates (from_date, to_date) - Sunday to Sunday\n",
    "# --- Complete list of the 17 week NFL schedule --- #\n",
    "search_date_ranges = [\n",
    "    #('2019-9-1','2019-9-8'),      # week 1\n",
    "    #('2019-9-8','2019-9-15'),     # week 2\n",
    "    #('2019-9-15','2019-9-22'),    # week 3\n",
    "    #('2019-9-22','2019-9-29'),    # week 4\n",
    "    #('2019-9-29','2019-10-6'),    # week 5\n",
    "    ('2019-10-6','2019-10-13'),   # week 6\n",
    "    ('2019-10-13','2019-10-20'),  # week 7\n",
    "    ('2019-10-20','2019-10-27'),  # week 8\n",
    "    ('2019-10-27','2019-11-3'),   # week 9\n",
    "    ('2019-11-3','2019-11-10'),   # week 10\n",
    "    ('2019-11-10','2019-11-17'),  # week 11\n",
    "    ('2019-11-17','2019-11-24'),  # week 12\n",
    "    ('2019-11-24','2019-12-1'),   # week 13\n",
    "    #('2019-12-1','2019-12-8'),    # week 14\n",
    "    #('2019-12-8','2019-12-15'),   # week 15\n",
    "    #('2019-12-15','2019-12-22'),  # week 16\n",
    "    #('2019-12-22','2019-12-29'),  # week 17\n",
    "                ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_str_date(str_date):\n",
    "    import time\n",
    "    day_of_week = str_date.split(' ')[0]\n",
    "    month = str_date.split(' ')[1]\n",
    "    day_of_month = str_date.split(' ')[2]\n",
    "    year = str_date.split(' ')[-1]\n",
    "    time_of_day = str_date.split(' ')[3]\n",
    "\n",
    "\n",
    "    new_str_date = f'{month} {day_of_month}, {year}'\n",
    "    ts = time.strptime(new_str_date, '%b %d, %Y')\n",
    "    new_ds_str = f'{ts.tm_year}-{ts.tm_mon}-{ts.tm_mday}'\n",
    "\n",
    "    return new_ds_str, time_of_day, str(ts.tm_year), str(ts.tm_mon), str(ts.tm_mday), str(ts.tm_wday)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test convert_str_date function - see how output is formatted\n",
    "#convert_str_date('Sat Nov 23 21:24:19 +0000 2019')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Function Description: \n",
    "'''\n",
    "def config_query(search_term,since=None,until=None,count=100,lang='en',result_type='mixed'):\n",
    "    \n",
    "    # query\n",
    "    search = {\n",
    "        'q':search_term,\n",
    "        'since':since,              # from_date\n",
    "        'until':until,              # Date format YYYY-MM-DD - returns tweets created before the given date\n",
    "        'lang':lang,\n",
    "        'result_type':result_type,    # mixed, recent, popular\n",
    "        'count':count,                # max is 100, defult is 15 per page\n",
    "\n",
    "        #'since_id': ,              # returns results with an ID more recent than the specified ID - if the limit of Tweets has occured since the since_id, the since_id will be forced to the oldest ID available\n",
    "        #'max_id': ,                # returns results with an ID older than or equal to the specified ID\n",
    "    }\n",
    "    \n",
    "    logger.debug(f'config_query: search:\\n{search}')\n",
    "    return search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Function: Cleans tweet text of URLs, HashTags and @Tags\n",
    "Returns: Cleaned text, lists of URLs, HasTags and @Tags\n",
    "'''\n",
    "def clean_tweet_text_meta(text):\n",
    "    \n",
    "    cleaned_text = []\n",
    "    new_text = None\n",
    "    urls = []\n",
    "    hash_tags = []\n",
    "    at_tags = []\n",
    "    \n",
    "    re_hash = re.compile('^#..+')\n",
    "    re_url = re.compile('^http*')\n",
    "    re_at = re.compile('^@.+')\n",
    "    punc_transtable = str.maketrans('', '', string.punctuation)\n",
    "    \n",
    "    # loop over tokens collecting meta info and cleaning text\n",
    "    if len(text) > 0:\n",
    "        tokens = text.split(' ')\n",
    "        logger.info(f'tokens: {tokens}')\n",
    "        \n",
    "        for tok in tokens:\n",
    "            \n",
    "            try:\n",
    "                # match Hash Tags\n",
    "                if re.match(re_hash,tok):\n",
    "                    hash_tags.append(tok.translate(punc_transtable))\n",
    "                    # hashtags have meaning in tweet text. remove the # character, keep the rest of the text\n",
    "                    cleaned_text.append(tok[1:])\n",
    "                # match URLs\n",
    "                elif re.match(re_url, tok):\n",
    "                    urls.append(tok)\n",
    "                # match @Tags\n",
    "                elif re.match(re_at,tok):\n",
    "                    at_tags.append(tok)\n",
    "                # keep text in original format for later cleaning techniques\n",
    "                else:\n",
    "                    cleaned_text.append(tok)\n",
    "                \n",
    "            except BaseException as be:\n",
    "                logger.warning(f'clean_tweet_text_meta: ***WARNING*** Caught BaseException: {be}')\n",
    "            \n",
    "    else:\n",
    "        logger.warning(f'clean_tweet_text_meta: ***WARNING***: text is empty: {text}')\n",
    "    \n",
    "    logger.info(f'clean_tweet_text_meta: cleaned tokens: {cleaned_text}')\n",
    "    # join cleaned text back together\n",
    "    new_text = ' '.join(cleaned_text)\n",
    "    \n",
    "    return new_text, urls, hash_tags, at_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Function Description: \n",
    "'''\n",
    "def page_search(twitter,query,text_file,raw_file):\n",
    "    results = twitter.cursor(twitter.search,**query, return_pages=True)\n",
    "    # search tweets\n",
    "    tweets_dict = {'id':[],'created_at':[],'date':[],'time':[],'user':[],'text':[],'favorite_count':[], 'year':[], 'month':[], 'day_of_month':[], 'day_of_week':[]}\n",
    "    tweets_text_metadata_dict = {'id':[],'date':[],'user':[],'urls':[],'hash_tags':[],'at_tags':[]}\n",
    "    page_cnt = 0\n",
    "    result_cnt = 0\n",
    "    with io.open(f'{text_file}', 'a',encoding='utf8') as f:\n",
    "        with io.open(f'{raw_file}','a',encoding='utf8') as r:\n",
    "            try: \n",
    "                # page is a list of twitter results\n",
    "                for i, page in enumerate(results):\n",
    "                    page_cnt +=1\n",
    "                    logger.info(f'Page: [{i}]')\n",
    "                    try:\n",
    "                        for j,result in enumerate(page):\n",
    "                            #logger.info(f'result type:{type(result)}')\n",
    "                            #break\n",
    "                            \n",
    "                            result_cnt += 1\n",
    "                            logger.info(f'Result: [{j}]')\n",
    "                            try:\n",
    "                                try:\n",
    "                                    logger.debug(f'{result[\"id_str\"]} | {result[\"user\"][\"screen_name\"]} | {result[\"created_at\"]} | {result[\"text\"]} | {result[\"user\"][\"favourites_count\"]}')\n",
    "                                except BaseException as be:\n",
    "                                    logger.warning(f'page_search: ***WARNING***: Caught BaseException writing debug log file: {be}')\n",
    "                                \n",
    "                                # dump raw tweet to file as json\n",
    "                                #raw_tweet = json.load(result)\n",
    "                                dump = json.dumps(result)\n",
    "                                r.write(dump)\n",
    "                                r.write('\\n')\n",
    "                                \n",
    "                                # dump tweet text to file\n",
    "                                f.write(f'{result[\"id_str\"]} {result[\"text\"]}')\n",
    "                                f.write('\\n')\n",
    "                                \n",
    "                                # add key attributes to tweets dictionary as return results\n",
    "                                tweets_dict['id'].append(result[\"id_str\"])\n",
    "                                tweets_dict['created_at'].append(result[\"created_at\"])\n",
    "                                tweets_dict['favorite_count'].append(result[\"user\"][\"favourites_count\"])\n",
    "                                        \n",
    "                                # call function to parse string date\n",
    "                                date_time = convert_str_date(result[\"created_at\"]) # get datetime components\n",
    "                                \n",
    "                                tweets_dict['date'].append(date_time[0])\n",
    "                                tweets_dict['time'].append(date_time[1])        \n",
    "                                tweets_dict['user'].append(result[\"user\"][\"screen_name\"])\n",
    "                                \n",
    "                                # call function to parse text for metadata\n",
    "                                clean_text = clean_tweet_text_meta(result[\"text\"])\n",
    "                        \n",
    "                                tweets_dict['text'].append(clean_text[0])\n",
    "                                        \n",
    "                                # create dictionary of tweet text metadata\n",
    "                                tweets_text_metadata_dict['id'].append(result[\"id_str\"])\n",
    "                                tweets_text_metadata_dict['date'].append(date_time[0])\n",
    "                                tweets_text_metadata_dict['user'].append(result[\"user\"][\"screen_name\"])\n",
    "                                tweets_text_metadata_dict['urls'].append(clean_text[1])\n",
    "                                tweets_text_metadata_dict['hash_tags'].append(clean_text[2])\n",
    "                                tweets_text_metadata_dict['at_tags'].append(clean_text[3])\n",
    "                                \n",
    "                                # track timeseries attributes for granular reporting and visualizations\n",
    "                                tweets_dict['year'].append(date_time[2])\n",
    "                                tweets_dict['month'].append(date_time[3])\n",
    "                                tweets_dict['day_of_month'].append(date_time[4])\n",
    "                                tweets_dict['day_of_week'].append(date_time[5])\n",
    "                                       \n",
    "                                #break\n",
    "\n",
    "                            except BaseException as be:\n",
    "                                logger.warning(f'**WARNING** Caught BaseException: {be}')\n",
    "                                             \n",
    "                    except BaseException as be:\n",
    "                        logger.warning(f'**WARNING** Caught BaseException: {be}')\n",
    "                    #break\n",
    "            except BaseException as be:\n",
    "                logger.warning(f'**WARNING** Caught BaseException: {be}')\n",
    "    \n",
    "    logger.info(f'page_search: processed page_cnt:[{page_cnt}] | total result_cnt: [{result_cnt}]')\n",
    "    \n",
    "    return pd.DataFrame.from_dict(tweets_dict), pd.DataFrame.from_dict(tweets_text_metadata_dict)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute Twitter Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Twython 3.6.0 reference documentation](https://twython.readthedocs.io/en/latest/api.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sets the search iteration for directory creation - this way we won't overwrite potentially useful data already created\n",
    "# set this once, then comment out\n",
    "search_iteration = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Execute Twitter search by pre-configured date ranges\n",
    "'''\n",
    "search_iteration +=1 # each time this block of code is ran, the search iteration will update and create a new output directory structure\n",
    "search_range_results_df = pd.DataFrame()\n",
    "search_tweets_text_meta_df = pd.DataFrame()\n",
    "\n",
    "# execute search by date ranges\n",
    "for dates in search_date_ranges:\n",
    "    search_range = f'{dates[0]}_{dates[1]}'\n",
    "    logger.info(f'search_range: {search_range}')\n",
    "    \n",
    "    # output file names based on date range search\n",
    "    outputPath = f'{dataDir}/{nfl_type}/{search_on}/v{search_iteration}/{search_range}'\n",
    "    if not os.path.exists(outputPath): os.makedirs(outputPath)\n",
    "        \n",
    "    tweet_filename=f'{outputPath}/tweet_text.txt'\n",
    "    raw_filename=f'{outputPath}/tweet_raw.txt'\n",
    "        \n",
    "    if not os.path.exists(f'{tweet_filename}'): open(f'{tweet_filename}', 'a').close()\n",
    "    if not os.path.exists(f'{raw_filename}'): open(f'{raw_filename}', 'a').close()\n",
    "    \n",
    "    \n",
    "    # configure query by dates\n",
    "    query = config_query(filtered_search_terms,since=dates[0],until=dates[1],count=2)\n",
    "    #break\n",
    "    \n",
    "    #------- EXECUTES TWITTER SEARCH -------------------#\n",
    "    result_df = page_search(py_tweets,query,tweet_filename,raw_filename) \n",
    "    \n",
    "    logger.debug(f'search result df: \\n{result_df}')\n",
    "    \n",
    "    # merge dataframes - complete table of search results collected and written out to csv file in code block below\n",
    "    search_range_results_df = search_range_results_df.append(result_df[0], ignore_index=True)\n",
    "    search_tweets_text_meta_df = search_tweets_text_meta_df.append(result_df[1], ignore_index=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_range_results_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_tweets_text_meta_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_search_df = search_range_results_df.sort_values(by=['month','day_of_month','day_of_week'], ascending=True)\n",
    "sorted_search_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(f'search_range_results_df shape: {search_range_results_df.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(f'x-rate-limit-remaining: [{py_tweets.get_lastfunction_header(\"x-rate-limit-remaining\")}]')\n",
    "logger.info(f'home_timeline: {py_tweets.get_home_timeline()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Full DataFrame of search results to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputPath = f'{dataDir}/{nfl_type}/{search_on}/v{search_iteration}'\n",
    "search_range_results_df.to_csv(f'{outputPath}/search_result_tweet_text_data.csv', index=False)\n",
    "search_tweets_text_meta_df.to_csv(f'{outputPath}/search_result_tweet_text_meta.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
