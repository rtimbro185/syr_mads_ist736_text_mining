{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Mine Twitter - NFL Search Patterns - PLAYER\n",
    "\n",
    "\n",
    "Ryan Timbrook (RTIMBROO)  \n",
    "DATE:11/30/2019 <br>\n",
    "Topic: Search Twitter for tweets on specific NFL Players, Coaches, and Teams\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Objective\n",
    "_____________________________________________________________________________________________\n",
    "Capture popular opinion of peoples tweets on certain NFL characters. \n",
    "Create a corpus of tweets for sentiment analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______________________________________________________________________________________________\n",
    "### Coding Environment Setup\n",
    "Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages for analysis and modeling\n",
    "import pandas as pd #data frame operations\n",
    "import numpy as np #arrays and math functions\n",
    "import requests\n",
    "import os\n",
    "import io\n",
    "import pickle\n",
    "import re\n",
    "import sys\n",
    "import string\n",
    "import json\n",
    "\n",
    "from os import path\n",
    "from datetime import date\n",
    "from datetime import time\n",
    "from datetime import datetime\n",
    "\n",
    "# Twython packages for twitter\n",
    "from twython import Twython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custome python packages\n",
    "import rtimbroo_utils as br             # custome python helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set global properties\n",
    "notebook_file_name = 'search_twitter_nfl_player_premium'\n",
    "report_file_name = 'search_twitter_nfl_player_premium'\n",
    "app_name = 'search_twitter_nfl_player_premium'\n",
    "log_level = 10 # 10-DEBUG, 20-INFO, 30-WARNING, 40-ERROR, 50-CRITICAL\n",
    "\n",
    "# setup working directory structure\n",
    "# set global properties\n",
    "dataDir = './data'\n",
    "outputDir = './output'\n",
    "configDir = './config'\n",
    "logOutDir = './logs'\n",
    "imageDir = './images'\n",
    "modelDir = './models'\n",
    "corpusDir = './corpus'\n",
    "# create base output directories if they don't exist\n",
    "if not os.path.exists(outputDir): os.mkdir(outputDir)\n",
    "if not os.path.exists(logOutDir): os.mkdir(logOutDir)\n",
    "if not os.path.exists(imageDir): os.mkdir(imageDir)\n",
    "if not os.path.exists(modelDir): os.mkdir(modelDir)\n",
    "if not os.path.exists(dataDir): os.mkdir(dataDir)\n",
    "if not os.path.exists(configDir): os.mkdir(configDir)\n",
    "if not os.path.exists(corpusDir): os.mkdir(corpusDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a logger for troubleshooting / data exploration\n",
    "logger = br.getFileLogger(logOutDir+'/',app_name,level=log_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get current date\n",
    "now = datetime.utcnow().isoformat()\n",
    "collection_date = re.findall('^[0-9]{4}-[0-9]{2}-[0-9]{2}',now)\n",
    "collection_date = collection_date[0]\n",
    "collection_date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. OBTAIN the data   \n",
    "________________________________________________________________________________________________\n",
    "Import external datasets for evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Twitter Search API Limits:\n",
    "[Sandbox Package](https://developer.twitter.com/en/pricing/search-fullarchive)\n",
    "\n",
    "* Time frame:\tFull history\n",
    "* Tweets per request:\t100\n",
    "* Counts vs. data:\tData only\n",
    "* Query length:\t128 characters\n",
    "* Operator availability:\tStandard\n",
    "* Rate limit per minute:\t30 requests/min\n",
    "* Enrichments:\tn/a\n",
    "* Dev environments:\t1\t\n",
    "* Monthly Tweet cap:\t5k\t\n",
    "* Rate limit per second: 10 requests/sec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate Twitter API Object\n",
    "Using Twython 3.6 Twitter API Wrapper\n",
    "[Twython 3.6.0 reference documentation](https://twython.readthedocs.io/en/latest/api.html)\n",
    "\n",
    "Twython, currently, has two main interfaces:\n",
    "\n",
    "* Twitter’s Core API (updating statuses, getting timelines, direct messaging, etc)\n",
    "* Twitter’s Streaming API<br>\n",
    "\n",
    "**Core Interface**<br>\n",
    "class twython.Twython(app_key=None, app_secret=None, oauth_token=None, oauth_token_secret=None, access_token=None, token_type='bearer', oauth_version=1, api_version='1.1', client_args=None, auth_endpoint='authenticate')\n",
    "\n",
    "__init__(app_key=None, app_secret=None, oauth_token=None, oauth_token_secret=None, access_token=None, token_type='bearer', oauth_version=1, api_version='1.1', client_args=None, auth_endpoint='authenticate')\n",
    "\n",
    "Parameters:\t\n",
    "* app_key – (optional) Your applications key\n",
    "* app_secret – (optional) Your applications secret key\n",
    "* oauth_token – (optional) When using OAuth 1\n",
    "* client_args – (optional) Accepts some requests Session parameters\n",
    "* auth_endpoint – (optional) Lets you select which authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load twitter credentials\n",
    "with open(f'{configDir}/twitter_credentials.json', 'r') as f:\n",
    "    tw_cred = json.load(f)\n",
    "\n",
    "# setup client header arguments to pass along to the API\n",
    "client_args = {\n",
    "    'headers':{\n",
    "        'User-Agent': 'AI_Public_Sentiment'\n",
    "    },\n",
    "    'timeout':300,\n",
    "    \n",
    "    \n",
    "}\n",
    "# test search urls\n",
    "base_url = \"https://api.twitter.com/1.1/tweets/search/\"\n",
    "\n",
    "# search_basic_standard_7day_free\n",
    "dev_7day_standard_url = \"https://api.twitter.com/1.1/tweets/search/tweets.json\"\n",
    "\n",
    "#  --- Premium API format: -- POST /search/:product/:label --- #\n",
    "# search_tweets_30_day_dev\n",
    "dev_30day_sandbox_url = \"https://api.twitter.com/1.1/tweets/search/30day/sandbox.json\"\n",
    "\n",
    "# search_tweets_fullarchive_dev\n",
    "dev_full_archive_url = \"https://api.twitter.com/1.1/tweets/search/fullarchive/devfullarchive.json\"\n",
    "\n",
    "# search_tweets_fullarchive_prod\n",
    "prod_full_archive_url = \"https://api.twitter.com/1.1/tweets/search/fullarchive/tweets.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Twython Object Resources:\n",
    "* [twython api.py](https://github.com/ryanmcgrath/twython/blob/master/twython/api.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate Twython object instance for premium search - key is the auth_endpoint equal to 'authorize'\n",
    "py_tweets_premium =  Twython(   \n",
    "    #app_key=tw_cred['CONSUMER_KEY'],\n",
    "    #app_secret=tw_cred['CONSUMER_SECRET'],\n",
    "    #oauth_token=tw_cred['ACCESS_TOKEN'],\n",
    "    #oauth_token_secret=tw_cred['ACCESS_SECRET'],\n",
    "    access_token=tw_cred['BEARER_TOKEN'],\n",
    "    token_type='bearer',\n",
    "    auth_endpoint='authorize',                  \n",
    "    oauth_version=2,\n",
    "    client_args=client_args)\n",
    "\n",
    "\n",
    "#logger.info(f'{test_twython.verify_credentials()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Search Terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set what to search on\n",
    "nfl_type = 'player'\n",
    "search_on = 'deshaun_watson'\n",
    "\n",
    "# setup base twitter search query\n",
    "search_terms=\"deshaun watson\"\n",
    "\n",
    "# add filters to search criteria\n",
    "#filtered_search_terms = search_terms + \" -filter:retweets\"\n",
    "\n",
    "#search_start_date = '2019-11-23' # limits to the last 7 days\n",
    "\n",
    "# number of tweets to return\n",
    "#num_tweets = 100 # sandbox rate limit - 100 tweets per request"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method\tDescription\n",
    "* POST /search/:product/:label\t\n",
    "    * Retrieve Tweets matching the specified query.\n",
    "* POST /search/:product/:label/counts\t\n",
    "    * Retrieve the number of Tweets matching the specified query.\n",
    "\n",
    "Where:\n",
    "* :product indicates the search endpoint you are making requests to, either 30day or fullarchive.\n",
    "* :label is the (case-sensitive) label associated with your search developer environment, as displayed at https://developer.twitter.com/en/account/environments.\n",
    "\n",
    "For example, if using the 30-day endpoint and your dev environment has a label of 'dev' (short for development), the search URLs would be:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure date search ranges\n",
    "# search dates (from_date, to_date) - Sunday to Sunday\n",
    "# --- Complete list of the 17 week NFL schedule --- #\n",
    "search_date_ranges = [\n",
    "    ('201909010000','201909080000'),      # week 1  --- dev_full_archive_url\n",
    "    ('201909080000','201909150000'),      # week 2  --- dev_full_archive_url\n",
    "    ('201909150000','201909220000'),      # week 3  --- dev_full_archive_url\n",
    "    ('201909220000','201909290000'),      # week 4  --- dev_full_archive_url\n",
    "    ('201909290000','201910060000'),      # week 5  --- dev_full_archive_url\n",
    "    ('201910060000','201910130000'),      # week 6  --- dev_full_archive_url\n",
    "    ('201910130000','201910200000'),      # week 7  --- dev_full_archive_url\n",
    "    ('201910200000','201910270000'),      # week 8  --- dev_full_archive_url\n",
    "    ('201910270000','201911030000'),      # week 9  --- dev_full_archive_url\n",
    "    ('201911030000','201911100000'),      # week 10 --- dev_30day_sandbox_url\n",
    "    ('201911100000','201911170000'),      # week 11 --- dev_30day_sandbox_url\n",
    "    ('201911170000','201911240000'),      # week 12 --- dev_30day_sandbox_url\n",
    "    ('201911240000','201912010000'),      # week 13 --- dev_30day_sandbox_url\n",
    "    #('201912010000','201912080000'),     # week 14 \n",
    "    #('201912080000','201912150000'),     # week 15\n",
    "    #('201912150000','201912220000'),     # week 16\n",
    "    #('201912220000','201912290000'),     # week 17\n",
    "                ]\n",
    "\n",
    "# 30day Search Range\n",
    "search_30day = search_date_ranges[-4:]\n",
    "logger.debug(f'{search_30day}')\n",
    "\n",
    "# full archive search range\n",
    "search_fullarchive = search_date_ranges[:len(search_date_ranges)-4]\n",
    "logger.debug(f'{search_fullarchive}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_str_date(str_date):\n",
    "    import time\n",
    "    day_of_week = str_date.split(' ')[0]\n",
    "    month = str_date.split(' ')[1]\n",
    "    day_of_month = str_date.split(' ')[2]\n",
    "    year = str_date.split(' ')[-1]\n",
    "    time_of_day = str_date.split(' ')[3]\n",
    "\n",
    "\n",
    "    new_str_date = f'{month} {day_of_month}, {year}'\n",
    "    ts = time.strptime(new_str_date, '%b %d, %Y')\n",
    "    new_ds_str = f'{ts.tm_year}-{ts.tm_mon}-{ts.tm_mday}'\n",
    "\n",
    "    return new_ds_str, time_of_day, str(ts.tm_year), str(ts.tm_mon), str(ts.tm_mday), str(ts.tm_wday)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert_str_date('Sat Nov 23 21:24:19 +0000 2019')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Function: used for standard basic search\n",
    "Returns:\n",
    "'''\n",
    "def config_query(search_term,since=None,until=None,count=100,lang='en',result_type='mixed'):\n",
    "    \n",
    "    # query\n",
    "    search = {\n",
    "        'q':search_term,\n",
    "        'since':since,              # from_date\n",
    "        'until':until,              # Date format YYYY-MM-DD - returns tweets created before the given date\n",
    "        'lang':lang,\n",
    "        'result_type':result_type,    # mixed, recent, popular\n",
    "        'count':count,                # max is 100, defult is 15 per page\n",
    "\n",
    "        #'since_id': ,              # returns results with an ID more recent than the specified ID - if the limit of Tweets has occured since the since_id, the since_id will be forced to the oldest ID available\n",
    "        #'max_id': ,                # returns results with an ID older than or equal to the specified ID\n",
    "    }\n",
    "    \n",
    "    logger.debug(f'config_query: search:\\n{search}')\n",
    "    return search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Util Function to build Twitter JSON API params\n",
    "'''\n",
    "def insert_str(string, str_to_insert, index):\n",
    "    return string[:index] + str_to_insert + string[index:]\n",
    "\n",
    "def make_search_params_str(q,fromDate,toDate,maxResults,next_page=None):\n",
    "    params=''\n",
    "    \n",
    "    if next_page is None:\n",
    "        params = '\\\"query\\\":\\\"{search_terms}\\\",\\\"fromDate\\\":\\\"{fromDate}\\\",\\\"toDate\\\":\\\"{toDate}\\\", \\\"maxResults\\\":{maxResults}'.format(\n",
    "            search_terms=q, \n",
    "            fromDate=fromDate, \n",
    "            toDate=toDate, \n",
    "            maxResults=maxResults,\n",
    "        )\n",
    "    else:\n",
    "        params = '\\\"query\\\":\\\"{search_terms}\\\",\\\"fromDate\\\":\\\"{fromDate}\\\",\\\"toDate\\\":\\\"{toDate}\\\", \\\"maxResults\\\":{maxResults},\\\"next\\\":\\\"{next_page}\\\"'.format(\n",
    "            search_terms=q, \n",
    "            fromDate=fromDate, \n",
    "            toDate=toDate, \n",
    "            maxResults=maxResults,\n",
    "            next_page=next_page\n",
    "        )\n",
    "    \n",
    "    params = insert_str(params,'{',0)\n",
    "    params = insert_str(params,'}',len(params))\n",
    "    \n",
    "    logger.debug(f'{params}')\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Function: Cleans tweet text of URLs, HashTags and @Tags\n",
    "Returns: Cleaned text, lists of URLs, HasTags and @Tags\n",
    "'''\n",
    "def clean_tweet_text_meta(text):\n",
    "    \n",
    "    cleaned_text = []\n",
    "    new_text = None\n",
    "    urls = []\n",
    "    hash_tags = []\n",
    "    at_tags = []\n",
    "    \n",
    "    re_hash = re.compile('^#..+')\n",
    "    re_url = re.compile('^http*')\n",
    "    re_at = re.compile('^@.+')\n",
    "    punc_transtable = str.maketrans('', '', string.punctuation)\n",
    "    \n",
    "    # loop over tokens collecting meta info and cleaning text\n",
    "    if len(text) > 0:\n",
    "        text = text.replace('\\n',' ')\n",
    "        tokens = text.split(' ')\n",
    "        logger.info(f'tokens: {tokens}')\n",
    "        \n",
    "        for tok in tokens:\n",
    "            \n",
    "            try:\n",
    "                # match Hash Tags\n",
    "                if re.match(re_hash,tok):\n",
    "                    hash_tags.append(tok.translate(punc_transtable))\n",
    "                    # hashtags have meaning in tweet text. remove the # character, keep the rest of the text\n",
    "                    cleaned_text.append(tok[1:])\n",
    "                # match URLs\n",
    "                elif re.match(re_url, tok):\n",
    "                    urls.append(tok)\n",
    "                # match @Tags\n",
    "                elif re.match(re_at,tok):\n",
    "                    at_tags.append(tok)\n",
    "                # keep text in original format for later cleaning techniques\n",
    "                else:\n",
    "                    cleaned_text.append(tok)\n",
    "                \n",
    "            except BaseException as be:\n",
    "                logger.warning(f'clean_tweet_text_meta: ***WARNING*** Caught BaseException: {be}')\n",
    "            \n",
    "    else:\n",
    "        logger.warning(f'clean_tweet_text_meta: ***WARNING***: text is empty: {text}')\n",
    "    \n",
    "    logger.debug(f'clean_tweet_text_meta: cleaned tokens: {cleaned_text}')\n",
    "    # join cleaned text back together\n",
    "    new_text = ' '.join(cleaned_text)\n",
    "    \n",
    "    return new_text, urls, hash_tags, at_tags\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test clean_tweet_text_v2\n",
    "#test_tweet = \"Attention, I LOVE WHAT I DO, Thats all Carry On!!! #bullsonparade. #Texans #1 in the Division Houston TEXANS @ Humbâ€¦ https://t.co/qZOYnP424X\"\n",
    "#test_tweet2 = \"@NYYfan2442 @_jliendro @OkcGhost @FieldYates Would be tough to beat KC but we could beat Houston. Not really impresâ€¦ https://t.co/pK2avVllxc\"\n",
    "#tw = clean_tweet_text_meta(test_tweet2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Twython Resource Docs:\n",
    "* [Setup Twython To Post Tweet And Search Tweets](https://code.luasoftware.com/tutorials/python/setup-twython-to-post-tweet-and-search-tweets/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#search_results = None\n",
    "def search_twitter(endpoint,params):\n",
    "    logger.debug(f'search_twitter: endpoint:{endpoint} | params:{params}')\n",
    "    search_results = py_tweets_premium.post(endpoint,params)\n",
    "    logger.debug(f'keys: {search_results.keys()}')\n",
    "    #return search_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Function Description: \n",
    "'''\n",
    "\n",
    "def page_search(endpoint,q,fromDate,toDate,maxResults,text_file,raw_file,next_page=None):\n",
    "    logger.debug(f'page_search: {endpoint} | {q} | {fromDate} | {toDate} | {maxResults} | {text_file} | {raw_file} | {next_page}')\n",
    "    \n",
    "    params = make_search_params_str(search_terms,fromDate,toDate,maxResults,next_page=None)\n",
    "\n",
    "    #search_results = search_twitter(endpoint,params)\n",
    "    _results = py_tweets_premium.post(endpoint,params)\n",
    "    logger.debug(f'page_search: result length: {len(_results[\"results\"])}')\n",
    "\n",
    "    \n",
    "    next_page = None\n",
    "    try:\n",
    "        next_page = _results['next']\n",
    "    except:\n",
    "        logger.debug(f'page_search: no next pages...')\n",
    "        #logger.info(f'next_page: {next_page}')\n",
    "    \n",
    "    # search tweets\n",
    "    tweets_dict = {'id':[],'created_at':[],'date':[],'time':[],'user':[],'text':[],'favorite_count':[], 'year':[], 'month':[], 'day_of_month':[], 'day_of_week':[]}\n",
    "    tweets_text_metadata_dict = {'id':[],'date':[],'user':[],'urls':[],'hash_tags':[],'at_tags':[]}\n",
    "    page_cnt = 0\n",
    "    result_cnt = 0\n",
    "    with io.open(f'{text_file}', 'a',encoding='utf8') as f:\n",
    "        with io.open(f'{raw_file}','a',encoding='utf8') as r:\n",
    "            try: \n",
    "                # page is a list of twitter results\n",
    "                for i, result in enumerate(_results['results']):\n",
    "                    page_cnt +=1\n",
    "                    logger.debug(f'Page: [{i}]')\n",
    "                    try:\n",
    "                        \n",
    "                        try:\n",
    "                            logger.debug(f'{result[\"id_str\"]} | {result[\"user\"][\"screen_name\"]} | {result[\"created_at\"]} | {result[\"text\"]} | {result[\"user\"][\"favourites_count\"]}')\n",
    "                        except BaseException as be:\n",
    "                            logger.warning(f'page_search: ***WARNING***: Caught BaseException writing debug log file: {be}')\n",
    "\n",
    "                        # dump raw tweet to file as json\n",
    "                        #raw_tweet = json.load(result)\n",
    "                        dump = json.dumps(result)\n",
    "                        r.write(dump)\n",
    "                        r.write('\\n')\n",
    "                                         \n",
    "                        # if tweet_mode='extended', use _result['full_text']\n",
    "                        text = ''\n",
    "                        try:\n",
    "                            text = result['retweeted_status'][\"extended_tweet\"]['full_text']\n",
    "                        except BaseException as be:\n",
    "                            logger.warning(f'page_search: NO full_text: {be}')\n",
    "                            text = result['text']\n",
    "\n",
    "                        # dump tweet text to file\n",
    "                        f.write(f'{result[\"id_str\"]} {text}')\n",
    "                        f.write('\\n')\n",
    "\n",
    "                        # add key attributes to tweets dictionary as return results\n",
    "                        tweets_dict['id'].append(result[\"id_str\"])\n",
    "                        tweets_dict['created_at'].append(result[\"created_at\"])\n",
    "                        tweets_dict['favorite_count'].append(result[\"user\"][\"favourites_count\"])\n",
    "\n",
    "                        # call function to parse string date\n",
    "                        date_time = convert_str_date(result[\"created_at\"]) # get datetime components\n",
    "\n",
    "                        tweets_dict['date'].append(date_time[0])\n",
    "                        tweets_dict['time'].append(date_time[1])        \n",
    "                        tweets_dict['user'].append(result[\"user\"][\"screen_name\"])\n",
    "\n",
    "                        # call function to parse text for metadata\n",
    "                        clean_text = clean_tweet_text_meta(text)\n",
    "\n",
    "                        tweets_dict['text'].append(clean_text[0])\n",
    "\n",
    "                        # create dictionary of tweet text metadata\n",
    "                        tweets_text_metadata_dict['id'].append(result[\"id_str\"])\n",
    "                        tweets_text_metadata_dict['date'].append(date_time[0])\n",
    "                        tweets_text_metadata_dict['user'].append(result[\"user\"][\"screen_name\"])\n",
    "                        tweets_text_metadata_dict['urls'].append(clean_text[1])\n",
    "                        tweets_text_metadata_dict['hash_tags'].append(clean_text[2])\n",
    "                        tweets_text_metadata_dict['at_tags'].append(clean_text[3])\n",
    "\n",
    "                        # track timeseries attributes for granular reporting and visualizations\n",
    "                        tweets_dict['year'].append(date_time[2])\n",
    "                        tweets_dict['month'].append(date_time[3])\n",
    "                        tweets_dict['day_of_month'].append(date_time[4])\n",
    "                        tweets_dict['day_of_week'].append(date_time[5])\n",
    "\n",
    "                        #break\n",
    "\n",
    "                    except BaseException as be:\n",
    "                        logger.warning(f'**WARNING** Caught BaseException: {be}')\n",
    "                                             \n",
    "            except BaseException as be:\n",
    "                logger.warning(f'**WARNING** Caught BaseException: {be}')\n",
    "    \n",
    "    logger.info(f'page_search: processed page_cnt:[{page_cnt}] | total result_cnt: [{result_cnt}] | next_page: {next_page}')\n",
    "    \n",
    "    return pd.DataFrame.from_dict(tweets_dict), pd.DataFrame.from_dict(tweets_text_metadata_dict), next_page\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute Twitter Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Twython 3.6.0 reference documentation](https://twython.readthedocs.io/en/latest/api.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sets the search iteration for directory creation - this way we won't overwrite potentially useful data already created\n",
    "# set this once, then comment out\n",
    "search_iteration = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute 30day Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Execute Twitter search by pre-configured date ranges\n",
    "------  30day Search ------\n",
    "\n",
    "'''\n",
    "# static search terms\n",
    "endpoint=dev_30day_sandbox_url\n",
    "search_terms = \"deshaun watson\"\n",
    "maxResults = 100\n",
    "\n",
    "search_iteration +=1 # each time this block of code is ran, the search iteration will update and create a new output directory structure\n",
    "search_range_results_df = pd.DataFrame()\n",
    "search_tweets_text_meta_df = pd.DataFrame()\n",
    "\n",
    "# execute search by date ranges\n",
    "for dates in search_30day:\n",
    "    search_range = f'{dates[0]}_{dates[1]}'\n",
    "    logger.info(f'search_range: {search_range}')\n",
    "    \n",
    "    # output file names based on date range search\n",
    "    outputPath = f'{dataDir}/{nfl_type}/{search_on}/v{search_iteration}/{search_range}'\n",
    "    if not os.path.exists(outputPath): os.makedirs(outputPath)\n",
    "        \n",
    "    tweet_filename=f'{outputPath}/tweet_text.txt'\n",
    "    raw_filename=f'{outputPath}/tweet_raw.txt'\n",
    "        \n",
    "    if not os.path.exists(f'{tweet_filename}'): open(f'{tweet_filename}', 'a').close()\n",
    "    if not os.path.exists(f'{raw_filename}'): open(f'{raw_filename}', 'a').close()\n",
    "  \n",
    "    \n",
    "    #------- EXECUTES TWITTER SEARCH -------------------#\n",
    "    result_df = page_search(endpoint,search_terms,dates[0],dates[1],maxResults,tweet_filename,raw_filename) \n",
    "    \n",
    "    # loop through pages\n",
    "    page_cnt = 0\n",
    "    while(len(result_df[2]) > 2):\n",
    "        page_cnt +=1\n",
    "        logger.info(f'next page search: page_cnt: {page_cnt}')\n",
    "        result_df = page_search(endpoint,search_terms,dates[0],dates[1],maxResults,tweet_filename,raw_filename,result_df[2]) \n",
    "        \n",
    "        # merge dataframes - complete table of search results collected and written out to csv file in code block below\n",
    "        search_range_results_df = search_range_results_df.append(result_df[0], ignore_index=True)\n",
    "        search_tweets_text_meta_df = search_tweets_text_meta_df.append(result_df[1], ignore_index=True)\n",
    "        if page_cnt == 4: break\n",
    "    \n",
    "    logger.debug(f'search result df: \\n{result_df}')\n",
    "    \n",
    "    # merge dataframes - complete table of search results collected and written out to csv file in code block below\n",
    "    search_range_results_df = search_range_results_df.append(result_df[0], ignore_index=True)\n",
    "    search_tweets_text_meta_df = search_tweets_text_meta_df.append(result_df[1], ignore_index=True)\n",
    "    \n",
    "    #break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute full archive search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Execute Twitter search by pre-configured date ranges\n",
    "------  Full Archive Search ------\n",
    "\n",
    "'''\n",
    "# static search terms\n",
    "endpoint=dev_full_archive_url\n",
    "search_terms = \"deshaun watson\"\n",
    "maxResults = 500\n",
    "\n",
    "#search_iteration +=1 # each time this block of code is ran, the search iteration will update and create a new output directory structure\n",
    "#search_range_results_df = pd.DataFrame()\n",
    "#search_tweets_text_meta_df = pd.DataFrame()\n",
    "\n",
    "# execute search by date ranges\n",
    "for dates in search_fullarchive:\n",
    "    search_range = f'{dates[0]}_{dates[1]}'\n",
    "    logger.info(f'search_range: {search_range}')\n",
    "    \n",
    "    # output file names based on date range search\n",
    "    outputPath = f'{dataDir}/{nfl_type}/{search_on}/v{search_iteration}/{search_range}'\n",
    "    if not os.path.exists(outputPath): os.makedirs(outputPath)\n",
    "        \n",
    "    tweet_filename=f'{outputPath}/tweet_text.txt'\n",
    "    raw_filename=f'{outputPath}/tweet_raw.txt'\n",
    "        \n",
    "    if not os.path.exists(f'{tweet_filename}'): open(f'{tweet_filename}', 'a').close()\n",
    "    if not os.path.exists(f'{raw_filename}'): open(f'{raw_filename}', 'a').close()\n",
    "    \n",
    "    #------- EXECUTES TWITTER SEARCH -------------------#\n",
    "    result_df = page_search(endpoint,search_terms,dates[0],dates[1],maxResults,tweet_filename,raw_filename) \n",
    "    \n",
    "    # loop through pages\n",
    "    page_cnt = 0\n",
    "    while(len(result_df[2]) > 2):\n",
    "        page_cnt +=1\n",
    "        logger.info(f'next page search: page_cnt: {page_cnt}')\n",
    "        result_df = page_search(endpoint,search_terms,dates[0],dates[1],maxResults,tweet_filename,raw_filename,result_df[2]) \n",
    "        \n",
    "        # merge dataframes - complete table of search results collected and written out to csv file in code block below\n",
    "        search_range_results_df = search_range_results_df.append(result_df[0], ignore_index=True)\n",
    "        search_tweets_text_meta_df = search_tweets_text_meta_df.append(result_df[1], ignore_index=True)\n",
    "        if page_cnt == 4: break\n",
    "    \n",
    "    logger.debug(f'search result df: \\n{result_df}')\n",
    "    \n",
    "    # merge dataframes - complete table of search results collected and written out to csv file in code block below\n",
    "    search_range_results_df = search_range_results_df.append(result_df[0], ignore_index=True)\n",
    "    search_tweets_text_meta_df = search_tweets_text_meta_df.append(result_df[1], ignore_index=True)\n",
    "    \n",
    "    #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_range_results_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_tweets_text_meta_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_search_df = search_range_results_df.sort_values(by=['month','day_of_month','day_of_week'], ascending=True)\n",
    "sorted_search_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(f'search_range_results_df shape: {search_range_results_df.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(f'x-rate-limit-remaining: [{py_tweets.get_lastfunction_header(\"x-rate-limit-remaining\")}]')\n",
    "logger.info(f'home_timeline: {py_tweets.get_home_timeline()}')          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Full DataFrame of search results to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputPath = f'{dataDir}/{nfl_type}/{search_on}/v{search_iteration}'\n",
    "search_range_results_df.to_csv(f'{outputPath}/search_result_tweet_text_data.csv', index=False)\n",
    "search_tweets_text_meta_df.to_csv(f'{outputPath}/search_result_tweet_text_meta.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
